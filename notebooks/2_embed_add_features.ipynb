{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed2b098",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c2f254",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T09:44:37.925796Z",
     "start_time": "2022-01-13T09:44:37.862295Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import string \n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "# from nltk.corpus import wordnet\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# #from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import spacy\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2444fd5",
   "metadata": {},
   "source": [
    "# Help function embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675b13f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T09:13:32.868682Z",
     "start_time": "2022-01-13T09:13:32.859560Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b10adf",
   "metadata": {},
   "source": [
    "# Load data from pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a101323",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T09:40:11.574515Z",
     "start_time": "2022-01-13T09:40:11.127834Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train\n",
    "with open('../raw_data/X_train_text', 'rb') as f1:\n",
    "    X_train_text = pickle.load(f1) \n",
    "with open('../raw_data/X_train_features', 'rb') as f1:\n",
    "    X_train_features = pickle.load(f1) \n",
    "\n",
    "# X_test\n",
    "with open('../raw_data/X_test_text', 'rb') as f1:\n",
    "    X_test_text = pickle.load(f1)   \n",
    "with open('../raw_data/X_test_features', 'rb') as f1:\n",
    "    X_test_features = pickle.load(f1)    \n",
    "\n",
    "# y_train\n",
    "with open('../raw_data/y_train', 'rb') as f1:\n",
    "    y_train = pickle.load(f1)\n",
    "    \n",
    "# y_test2\n",
    "with open('../raw_data/y_test', 'rb') as f1:\n",
    "    y_test = pickle.load(f1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337a3b56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T09:40:26.844561Z",
     "start_time": "2022-01-13T09:40:26.840748Z"
    }
   },
   "outputs": [],
   "source": [
    "assert(len(X_train_text)==len(y_train))\n",
    "assert(len(X_test_text)==len(y_test))\n",
    "assert(X_train_features.shape[0]==len(y_train))\n",
    "assert(X_test_features.shape[0])==len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3183452",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T09:41:39.107800Z",
     "start_time": "2022-01-13T09:41:39.100836Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"########## train #########\")\n",
    "print(len(y_train))\n",
    "print(len(X_train_text))\n",
    "print(X_train_features.shape)\n",
    "\n",
    "print(\"########## test #########\")\n",
    "print(len(y_test))\n",
    "print(len(X_test_text))\n",
    "print(X_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce37303",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e43af31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T09:45:36.294092Z",
     "start_time": "2022-01-13T09:45:06.290405Z"
    }
   },
   "outputs": [],
   "source": [
    "print(list(api.info()['models'].keys()))\n",
    "word2vec_transf = api.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9898d23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T09:47:33.502969Z",
     "start_time": "2022-01-13T09:47:33.495553Z"
    }
   },
   "outputs": [],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67a421",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T09:47:47.911220Z",
     "start_time": "2022-01-13T09:47:47.061446Z"
    }
   },
   "outputs": [],
   "source": [
    "del word2vec_transfimporer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e57ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T09:49:14.325552Z",
     "start_time": "2022-01-13T09:49:14.067692Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "## Free up memory space by deleting + gc.collect()\n",
    "#del df, X_train_embed, X_test_embed, X_train, X_test,word2vec_transfer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be3eb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T09:49:24.857670Z",
     "start_time": "2022-01-13T09:49:24.846026Z"
    }
   },
   "outputs": [],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290ee17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T09:44:15.271091Z",
     "start_time": "2022-01-13T09:43:43.496823Z"
    }
   },
   "outputs": [],
   "source": [
    "# Embed the training and test sentences\n",
    "X_train_embed_tf = embedding(word2vec_transfer, X_train_text)\n",
    "X_test_embed_tf = embedding(word2vec_transfer, X_test_text)\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad = pad_sequences(X_train_embed_tf, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad = pad_sequences(X_test_embed_tf, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1226f631",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
