{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d923a3d",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465bbf60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T15:00:06.966128Z",
     "start_time": "2022-01-12T15:00:02.569230Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import string \n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "# from nltk.corpus import wordnet\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# #from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import spacy\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from sklearn.utils import resample\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fd507c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T15:01:01.712530Z",
     "start_time": "2022-01-12T15:01:00.277699Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 16:01:00.444011: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-12 16:01:00.444029: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d43433",
   "metadata": {},
   "source": [
    "# Help functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b37a6",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c67b12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:50:25.109248Z",
     "start_time": "2022-01-12T14:50:25.104629Z"
    }
   },
   "outputs": [],
   "source": [
    "def undersample(df, prop = 2):\n",
    "    \n",
    "    # get label 2\n",
    "    df2 = df[df[\"label\"]==2]\n",
    "    n2 = df2.shape[0]\n",
    "    \n",
    "    # get label 0\n",
    "    df0= df[df[\"label\"]==0]\n",
    "    df0 = df0.sample(frac=1)\n",
    "    \n",
    "    # sample from label 0\n",
    "    n0 = round(prop*n2)\n",
    "    df0_sample = df0.head(n0)\n",
    "    \n",
    "    # concatenate\n",
    "    df = pd.concat([df0_sample,df2], axis = 0)\n",
    "    df = df.sample(frac=1)\n",
    "    df.reset_index(inplace = True)\n",
    "    \n",
    "    assert(n0==prop*n2)\n",
    "    return df[[\"text\",\"score_punctuation\", \"score_capital_word\", \"label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d508f26",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5297fb70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:50:25.115329Z",
     "start_time": "2022-01-12T14:50:25.110934Z"
    }
   },
   "outputs": [],
   "source": [
    "def oversample(df, prop = 0.3 , seed = 123):\n",
    "    n = df.shape[0]\n",
    "    n2_new = round(prop*n) \n",
    "    \n",
    "    # get label 2\n",
    "    df2 = df[df[\"label\"]==2]\n",
    "    n2 = df2.shape[0]\n",
    "    n2_resample = n2_new - n2\n",
    "    if (n2_resample) < 0:\n",
    "        print(f\"WARNING: proportion {prop} already satisfied, DF unchanged\")\n",
    "    \n",
    "    else : \n",
    "        indices = list(range(n2))\n",
    "    \n",
    "        # resample from df2\n",
    "        indices_resample = resample(indices, replace=True, n_samples =n2_resample, random_state=f\"{seed}\")\n",
    "        df2_resample = df2.iloc[indices_resample]\n",
    "\n",
    "        df2_new = pd.concat([df2_resample, df2], axis =0)\n",
    "\n",
    "        df_new = pd.concat([df2_new, df[df[\"label\"]==0]], axis =0)\n",
    "        df = df_new\n",
    "    #assert(df2_new.shape[0]==prop*n)\n",
    "    df = df.sample(frac=1)\n",
    "    df.reset_index(inplace = True)\n",
    "    return df[[\"text\",\"score_punctuation\", \"score_capital_word\", \"label\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73ea6e",
   "metadata": {},
   "source": [
    "## Data cleaning and tokenizing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af859ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:50:25.121023Z",
     "start_time": "2022-01-12T14:50:25.117735Z"
    }
   },
   "outputs": [],
   "source": [
    "# # clean each tweet\n",
    "# def clean_data(data, remove_special_char_2lower_case = True):\n",
    "    \n",
    "#     #Removing URLs with a regular expression\n",
    "#     url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "#     data = url_pattern.sub(r'', data)\n",
    "\n",
    "#     # Remove Emails\n",
    "#     data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "   \n",
    "#     # tokenize + remove scpecial characters + set to lower case\n",
    "#     if remove_special_char_2lower_case == True:\n",
    "#         data = text_to_word_sequence(data) \n",
    "#     else :\n",
    "#     # tokenize \n",
    "#         data = data.split() \n",
    "    \n",
    "    \n",
    "#     # Remove stopwords\n",
    "#     stop_words = set(stopwords.words('english')) \n",
    "#     data = [w for w in data if not w in stop_words]         \n",
    "    \n",
    "#     # Remove digits\n",
    "#     data = ' '.join(word for word in data if not word.isdigit())\n",
    "    \n",
    "    \n",
    "#     return text_to_word_sequence(data)\n",
    "\n",
    "# # clean dataset\n",
    "# def apply_data_cleaning(X, text, drop_text = False, remove_special_char_2lower_case = True):\n",
    "#     ln = X.shape[0]\n",
    "#     sentences = []\n",
    "#     for i in range(ln):\n",
    "#         tmp = X.iloc[i][f'{text}']\n",
    "#         tmp_clean = clean_data(tmp,remove_special_char_2lower_case = f\"{remove_special_char_2lower_case}\")\n",
    "#         sentences.append(tmp_clean)\n",
    "#     X[\"sentences\"] = sentences\n",
    "#     if drop_text == True:\n",
    "#         X.drop(columns = f'{text}', inplace = True)\n",
    "#     return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ad55d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:50:25.130034Z",
     "start_time": "2022-01-12T14:50:25.122926Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \n",
    "    #Removing URLs with a regular expression\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data = url_pattern.sub(r'', data)\n",
    "\n",
    "    # Remove Emails\n",
    "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "   \n",
    "    # tokenize + remove scpecial characters + set to lower case\n",
    "    data = text_to_word_sequence(data) \n",
    "    \n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    data = [w for w in data if not w in stop_words]         \n",
    "    \n",
    "    # Remove digits\n",
    "    data = ' '.join(word for word in data if not word.isdigit())\n",
    "    \n",
    "    \n",
    "    return text_to_word_sequence(data)\n",
    "\n",
    "def apply_data_cleaning(X, text, drop_text = False):\n",
    "    ln = X.shape[0]\n",
    "    sentences = []\n",
    "    for i in range(ln):\n",
    "       # print (f\"{i}\")\n",
    "        tmp = X.iloc[i][f'{text}']\n",
    "        tmp_clean = clean_data(tmp)\n",
    "        sentences.append(tmp_clean)\n",
    "    X[\"sentences\"] = sentences\n",
    "    if drop_text == True:\n",
    "        X.drop(columns = f'{text}', inplace = True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c909fb53",
   "metadata": {},
   "source": [
    "## Help function for Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cbc1bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:50:25.139345Z",
     "start_time": "2022-01-12T14:50:25.133608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96b88e2",
   "metadata": {},
   "source": [
    "# Help function adding features to embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476200f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:50:25.146304Z",
     "start_time": "2022-01-12T14:50:25.140787Z"
    }
   },
   "outputs": [],
   "source": [
    "def append_features_to_tensor(X_text, X_feature, _max ):\n",
    "\n",
    "    tmp = np.empty(shape=(X_text.shape[0],X_text.shape[1]+2, X_text.shape[2] ))\n",
    "    tmp[:,0:X_text.shape[1],:] = X_text\n",
    "\n",
    "    first_indices = X_text.shape[0] ## # of twitts \n",
    "    tmp_punct =  np.zeros(shape = (1,X_text.shape[2]))\n",
    "    tmp_cap = np.zeros(shape = (1,X_text.shape[2]))\n",
    "\n",
    "    for i in range(first_indices):\n",
    "        if X_feature.iloc[i,0] == 1:\n",
    "            tmp_punct =  tmp_punct*(_max+0.5)\n",
    "\n",
    "        if X_feature.iloc[i,1] == 1:\n",
    "            tmp_cap = tmp_cap*(_max+1)   \n",
    "\n",
    "        tmp[i,200,:] = tmp_punct\n",
    "        tmp[i,201,:] = tmp_cap\n",
    "\n",
    "    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba5b895",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63050ed",
   "metadata": {},
   "source": [
    "## Load re-labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852722db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:50:26.225627Z",
     "start_time": "2022-01-12T14:50:25.777374Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../raw_data/DF', 'rb') as handle:\n",
    "    DF = pickle.load(handle)\n",
    "DF.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6946bb7",
   "metadata": {},
   "source": [
    "## Undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6cea58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:50:26.789524Z",
     "start_time": "2022-01-12T14:50:26.573109Z"
    }
   },
   "outputs": [],
   "source": [
    "df = undersample(df= DF, prop = 9)\n",
    "df.shape\n",
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d31c95",
   "metadata": {},
   "source": [
    "## Oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35ff30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:50:27.402650Z",
     "start_time": "2022-01-12T14:50:27.398833Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = oversample(df =df , prop = 0.3 , seed = 123)\n",
    "# df.shape\n",
    "# df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf97f55",
   "metadata": {},
   "source": [
    "## SubSampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67210a4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:50:27.922141Z",
     "start_time": "2022-01-12T14:50:27.914723Z"
    }
   },
   "outputs": [],
   "source": [
    "N =100\n",
    "df = df.head(N)\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ae9f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T20:19:51.670017Z",
     "start_time": "2022-01-11T20:19:51.664240Z"
    }
   },
   "source": [
    "# Data split train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ea20a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:50:28.393916Z",
     "start_time": "2022-01-12T14:50:28.379899Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split in X, y\n",
    "X = df.copy() \n",
    "X.drop(columns = \"label\", inplace = True)\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=42)\n",
    "assert(len(X_train)==len(y_train))\n",
    "assert(len(X_test)==len(y_test))\n",
    "assert(len(X_train)+len(X_test)==X.shape[0])\n",
    "\n",
    "# Split X into text and features\n",
    "X_train_text = X_train[[\"text\"]]\n",
    "X_test_text = X_test[[\"text\"]]\n",
    "\n",
    "X_train_features = X_train[[\"score_punctuation\", \"score_capital_word\"]]\n",
    "X_test_features = X_test[[\"score_punctuation\", \"score_capital_word\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d01cd11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:50:28.643807Z",
     "start_time": "2022-01-12T14:50:28.636282Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(y))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fbdd50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:50:28.886896Z",
     "start_time": "2022-01-12T14:50:28.882904Z"
    }
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84641d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:50:29.143318Z",
     "start_time": "2022-01-12T14:50:29.133561Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424dcd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:50:29.341587Z",
     "start_time": "2022-01-12T14:50:29.332710Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9253cf99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:11:24.757485Z",
     "start_time": "2022-01-11T21:11:24.748902Z"
    }
   },
   "source": [
    "# Clean and tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e64cb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:50:30.286814Z",
     "start_time": "2022-01-12T14:50:30.226798Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(X_text.head())\n",
    "X_train_text = apply_data_cleaning(X = X_train_text, text = \"text\",drop_text = True)#, remove_special_char_2lower_case = True)\n",
    "X_test_text = apply_data_cleaning(X = X_test_text, text = \"text\", drop_text = True)#, remove_special_char_2lower_case = True)\n",
    "\n",
    "X_train_text = list(X_train_text.sentences)\n",
    "y_train = np.array(y)\n",
    "\n",
    "X_test_text = list(X_test_text.sentences)\n",
    "y_test = np.array(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a56f308",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:25:31.541945Z",
     "start_time": "2022-01-11T21:25:31.516103Z"
    }
   },
   "source": [
    "# Embedding TF\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd0543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:11.109445Z",
     "start_time": "2022-01-12T14:50:31.498799Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(list(api.info()['models'].keys()))\n",
    "ll = ['fasttext-wiki-news-subwords-300',\n",
    " 'conceptnet-numberbatch-17-06-300', \n",
    " 'word2vec-ruscorpora-300', \n",
    " 'word2vec-google-news-300', \n",
    " 'glove-wiki-gigaword-50', \n",
    " 'glove-wiki-gigaword-100',\n",
    " 'glove-wiki-gigaword-200', \n",
    " 'glove-wiki-gigaword-300', \n",
    " 'glove-twitter-25', \n",
    " 'glove-twitter-50', \n",
    " 'glove-twitter-100', \n",
    " 'glove-twitter-200', \n",
    " '__testing_word2vec-matrix-synopsis']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4eb4f1e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T17:15:55.427594Z",
     "start_time": "2022-01-12T17:15:55.044101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mimi/code/hayatMohaAl/project_BAN/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../raw_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3dc721",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:12.361634Z",
     "start_time": "2022-01-12T14:51:11.111230Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../raw_data/word2vec_transfer', 'wb') as f1:\n",
    "    pickle.dump(word2vec_transfer, f1, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8bf12a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:12.393338Z",
     "start_time": "2022-01-12T14:51:12.367122Z"
    }
   },
   "outputs": [],
   "source": [
    "# Embed the training and test sentences\n",
    "X_train_embed_tf = embedding(word2vec_transfer, X_train_text)\n",
    "X_test_embed_tf = embedding(word2vec_transfer, X_test_text)\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad = pad_sequences(X_train_embed_tf, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad = pad_sequences(X_test_embed_tf, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b9e743",
   "metadata": {},
   "source": [
    "## Pickle and save embedded data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778daab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:12.402400Z",
     "start_time": "2022-01-12T14:51:12.395187Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(y_train))\n",
    "print(len(y_test))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b482999",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:12.420853Z",
     "start_time": "2022-01-12T14:51:12.404648Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train\n",
    "with open('../raw_data/X_train_pad', 'wb') as f1:\n",
    "    pickle.dump(X_train_pad, f1)\n",
    "    \n",
    "# X_test\n",
    "with open('../raw_data/X_test_pad', 'wb') as f1:\n",
    "    pickle.dump(X_test_pad, f1)   \n",
    "    \n",
    "\n",
    "# y_train\n",
    "with open('../raw_data/y_train', 'wb') as f1:\n",
    "    pickle.dump(y_train, f1)\n",
    "    \n",
    "# y_test2\n",
    "with open('../raw_data/y_test', 'wb') as f1:\n",
    "    pickle.dump(y_test, f1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c3d0bf",
   "metadata": {},
   "source": [
    "# Add features to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da415f26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:12.513268Z",
     "start_time": "2022-01-12T14:51:12.504311Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pad_added_features = append_features_to_tensor(X_train_pad, \\\n",
    "                                                       X_feature = X_train_features, \\\n",
    "                                                       _max = X_train_pad.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c8f2bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:12.518953Z",
     "start_time": "2022-01-12T14:51:12.514523Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_pad_added_features = append_features_to_tensor(X_test_pad, \\\n",
    "                                                       X_feature = X_test_features, \\\n",
    "                                                       _max = X_test_pad.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61000c3b",
   "metadata": {},
   "source": [
    "# Pickle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a89a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cba5fc1",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b00d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:12.733464Z",
     "start_time": "2022-01-12T14:51:12.521345Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Masking())#, label =\"layer1_masking\")\n",
    "    model.add(layers.LSTM(20, activation='tanh'))#, label =\"layer2_LSTM\")\n",
    "    model.add(layers.Dense(15, activation='relu'))#, label =\"layer3_dense_relu\")\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))#, label =\"layer4_dense_sigmoid\")\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=[\"Recall\"]) #['recall']) # tp/(tp+fn) metrics=[tf.keras.metrics.Recall()]\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = init_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a924c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:19.233118Z",
     "start_time": "2022-01-12T14:51:12.735028Z"
    }
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "#%%time\n",
    "history = model.fit(X_train_pad, y_train, \n",
    "          batch_size = 32,  # low --> optimizes on smaller ss --> faster --> but to generalize \n",
    "          epochs=10,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed36f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:19.242881Z",
     "start_time": "2022-01-12T14:51:19.234952Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss_recall(history, title=None):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "    \n",
    "    # --- LOSS --- \n",
    "    \n",
    "    ax[0].plot(history.history['loss'])\n",
    "    ax[0].plot(history.history['val_loss'])\n",
    "    ax[0].set_title('Model loss')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylim((0,3))\n",
    "    ax[0].legend(['Train', 'Test'], loc='best')\n",
    "    ax[0].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[0].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    # --- ACCURACY\n",
    "    \n",
    "    ax[1].plot(history.history['recall'])\n",
    "    ax[1].plot(history.history['val_recall'])\n",
    "    ax[1].set_title('Model Recall')\n",
    "    ax[1].set_ylabel('recall')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Test'], loc='best')\n",
    "    ax[1].set_ylim((0,1))\n",
    "    ax[1].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[1].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    if title:\n",
    "        fig.suptitle(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a5ddc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:19.718879Z",
     "start_time": "2022-01-12T14:51:19.244561Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss_recall(model.history, title=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b808d",
   "metadata": {},
   "source": [
    "## Select epoch and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fc62cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:19.729023Z",
     "start_time": "2022-01-12T14:51:19.723503Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# filepath = './saved_model'\n",
    "# save_model(model, filepath)\n",
    "# model = load_model(filepath, compile = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847839fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:52:01.914158Z",
     "start_time": "2022-01-12T14:51:56.574994Z"
    }
   },
   "outputs": [],
   "source": [
    "model = init_model()\n",
    "model.fit(X_train_pad, y_train, \n",
    "          batch_size = 32,  # low --> optimizes on smaller ss --> faster --> but to generalize \n",
    "          epochs=10,\n",
    "          validation_split=0.3#,\n",
    "        #  callbacks=[es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7915077b",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f06f30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:54:00.494405Z",
     "start_time": "2022-01-12T14:54:00.330461Z"
    }
   },
   "outputs": [],
   "source": [
    "! ls ../project_BAN/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdeb6c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:54:07.608981Z",
     "start_time": "2022-01-12T14:54:03.917125Z"
    }
   },
   "outputs": [],
   "source": [
    "models.save_model(model, '../project_BAN/models/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1a7164",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1959a0df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T15:01:09.442064Z",
     "start_time": "2022-01-12T15:01:07.283254Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 16:01:07.335109: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-01-12 16:01:07.335130: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-01-12 16:01:07.335148: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pop-os): /proc/driver/nvidia/version does not exist\n",
      "2022-01-12 16:01:07.335357: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-12 16:01:08.294077: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2022-01-12 16:01:08.303399: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2022-01-12 16:01:08.455392: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2022-01-12 16:01:08.464754: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2022-01-12 16:01:08.600901: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2022-01-12 16:01:08.610349: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = models.load_model('../project_BAN/models/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136fd14f",
   "metadata": {},
   "source": [
    "# Predict from tweet - test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7a0c8",
   "metadata": {},
   "source": [
    "### Prediction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e4b67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:44:45.115240Z",
     "start_time": "2022-01-12T14:44:45.083407Z"
    }
   },
   "outputs": [],
   "source": [
    "# write tweet\n",
    "tweet = \"I love deep learning\"\n",
    "\n",
    "score_punctuation = ponctuation_check(tweet)\n",
    "score_capitalization = capital_word_check(tweet)\n",
    "\n",
    "tweet_tokenized = clean_data(tweet)\n",
    "tweet_tokenized\n",
    "\n",
    "tweet_emebedded = embed_sentence_with_TF(word2vec_transfer, tweet_tokenized)\n",
    "tweet_emebedded_reshape  = tweet_emebedded.reshape((1,tweet_emebedded.shape[0], tweet_emebedded.shape[1]))\n",
    "\n",
    "res = loaded_model.predict(tweet_reshape)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62662bf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:44:36.946229Z",
     "start_time": "2022-01-12T14:44:36.946209Z"
    }
   },
   "outputs": [],
   "source": [
    "if res[0][0]< 0.5:\n",
    "    print(\"good tweet\")\n",
    "else:\n",
    "    print(\"hate tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c73f6e44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T08:19:48.084533Z",
     "start_time": "2022-01-13T08:19:00.680428Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import pickle\n",
    "word2vec_transfer = api.load('glove-twitter-50')\n",
    "with open('../project_BAN/models/word2vec_transfer', 'wb') as f1:\n",
    "    pickle.dump(word2vec_transfer, f1, protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f033fcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T08:20:25.125233Z",
     "start_time": "2022-01-13T08:20:23.088112Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../project_BAN/models/word2vec_transfer', 'wb') as f1:\n",
    "    pickle.dump(word2vec_transfer, f1, protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f5521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../project_BAN/models/word2vec_transfer', 'rb') as handle:\n",
    "   #     word2vec_transfer = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "625.455px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
