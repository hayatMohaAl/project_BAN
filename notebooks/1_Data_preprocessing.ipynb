{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d923a3d",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "465bbf60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:22:30.935600Z",
     "start_time": "2022-01-11T21:22:26.564597Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-11 22:22:28.845558: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-11 22:22:28.845576: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import string \n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "# from nltk.corpus import wordnet\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# #from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import spacy\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.utils import resample\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d43433",
   "metadata": {},
   "source": [
    "# Help functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b37a6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c67b12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:22:30.948209Z",
     "start_time": "2022-01-11T21:22:30.945031Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def undersample(df, prop = 2):\n",
    "    \n",
    "    # get label 2\n",
    "    df2 = df[df[\"label\"]==2]\n",
    "    n2 = df2.shape[0]\n",
    "    \n",
    "    # get label 0\n",
    "    df0= df[df[\"label\"]==0]\n",
    "    df0 = df0.sample(frac=1)\n",
    "    \n",
    "    # sample from label 0\n",
    "    n0 = round(prop*n2)\n",
    "    df0_sample = df0.head(n0)\n",
    "    \n",
    "    # concatenate\n",
    "    df = pd.concat([df0_sample,df2], axis = 0)\n",
    "    df = df.sample(frac=1)\n",
    "    df.reset_index(inplace = True)\n",
    "    \n",
    "    assert(n0==prop*n2)\n",
    "    return df[[\"text\",\"score_punctuation\", \"score_capital_word\", \"label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d508f26",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5297fb70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:22:30.956756Z",
     "start_time": "2022-01-11T21:22:30.949911Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def oversample(df, prop = 0.3 , seed = 123):\n",
    "    n = df.shape[0]\n",
    "    n2_new = round(prop*n) \n",
    "    \n",
    "    # get label 2\n",
    "    df2 = df[df[\"label\"]==2]\n",
    "    n2 = df2.shape[0]\n",
    "    n2_resample = n2_new - n2\n",
    "    if (n2_resample) < 0:\n",
    "        print(f\"WARNING: proportion {prop} already satisfied, DF unchanged\")\n",
    "    \n",
    "    else : \n",
    "        indices = list(range(n2))\n",
    "    \n",
    "        # resample from df2\n",
    "        indices_resample = resample(indices, replace=True, n_samples =n2_resample, random_state=f\"{seed}\")\n",
    "        df2_resample = df2.iloc[indices_resample]\n",
    "\n",
    "        df2_new = pd.concat([df2_resample, df2], axis =0)\n",
    "\n",
    "        df_new = pd.concat([df2_new, df[df[\"label\"]==0]], axis =0)\n",
    "        df = df_new\n",
    "    #assert(df2_new.shape[0]==prop*n)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73ea6e",
   "metadata": {},
   "source": [
    "## Data cleaning and tokenizing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3af859ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:22:30.965184Z",
     "start_time": "2022-01-11T21:22:30.961813Z"
    }
   },
   "outputs": [],
   "source": [
    "# # clean each tweet\n",
    "# def clean_data(data, remove_special_char_2lower_case = True):\n",
    "    \n",
    "#     #Removing URLs with a regular expression\n",
    "#     url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "#     data = url_pattern.sub(r'', data)\n",
    "\n",
    "#     # Remove Emails\n",
    "#     data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "   \n",
    "#     # tokenize + remove scpecial characters + set to lower case\n",
    "#     if remove_special_char_2lower_case == True:\n",
    "#         data = text_to_word_sequence(data) \n",
    "#     else :\n",
    "#     # tokenize \n",
    "#         data = data.split() \n",
    "    \n",
    "    \n",
    "#     # Remove stopwords\n",
    "#     stop_words = set(stopwords.words('english')) \n",
    "#     data = [w for w in data if not w in stop_words]         \n",
    "    \n",
    "#     # Remove digits\n",
    "#     data = ' '.join(word for word in data if not word.isdigit())\n",
    "    \n",
    "    \n",
    "#     return text_to_word_sequence(data)\n",
    "\n",
    "# # clean dataset\n",
    "# def apply_data_cleaning(X, text, drop_text = False, remove_special_char_2lower_case = True):\n",
    "#     ln = X.shape[0]\n",
    "#     sentences = []\n",
    "#     for i in range(ln):\n",
    "#         tmp = X.iloc[i][f'{text}']\n",
    "#         tmp_clean = clean_data(tmp,remove_special_char_2lower_case = f\"{remove_special_char_2lower_case}\")\n",
    "#         sentences.append(tmp_clean)\n",
    "#     X[\"sentences\"] = sentences\n",
    "#     if drop_text == True:\n",
    "#         X.drop(columns = f'{text}', inplace = True)\n",
    "#     return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ad55d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:22:30.974140Z",
     "start_time": "2022-01-11T21:22:30.967036Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \n",
    "    #Removing URLs with a regular expression\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data = url_pattern.sub(r'', data)\n",
    "\n",
    "    # Remove Emails\n",
    "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "   \n",
    "    # tokenize + remove scpecial characters + set to lower case\n",
    "    data = text_to_word_sequence(data) \n",
    "    \n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    data = [w for w in data if not w in stop_words]         \n",
    "    \n",
    "    # Remove digits\n",
    "    data = ' '.join(word for word in data if not word.isdigit())\n",
    "    \n",
    "    \n",
    "    return text_to_word_sequence(data)\n",
    "\n",
    "def apply_data_cleaning(X, text, drop_text = False):\n",
    "    ln = X.shape[0]\n",
    "    sentences = []\n",
    "    for i in range(ln):\n",
    "       # print (f\"{i}\")\n",
    "        tmp = X.iloc[i][f'{text}']\n",
    "        tmp_clean = clean_data(tmp)\n",
    "        sentences.append(tmp_clean)\n",
    "    X[\"sentences\"] = sentences\n",
    "    if drop_text == True:\n",
    "        X.drop(columns = f'{text}', inplace = True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c909fb53",
   "metadata": {},
   "source": [
    "## Help function for Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cbc1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed_2 = embedding(word2vec_transfer, X_train_text)\n",
    "X_test_embed_2 = embedding(word2vec_transfer, X_test_text)\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad_2 = pad_sequences(X_train_embed_2, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad_2 = pad_sequences(X_test_embed_2, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba5b895",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63050ed",
   "metadata": {},
   "source": [
    "## Load re-labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "852722db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:22:31.771381Z",
     "start_time": "2022-01-11T21:22:30.976087Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../raw_data/DF', 'rb') as handle:\n",
    "    DF = pickle.load(handle)\n",
    "DF.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6946bb7",
   "metadata": {},
   "source": [
    "## Undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e6cea58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:22:32.324344Z",
     "start_time": "2022-01-11T21:22:31.773166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20182\n",
       "2    10091\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = undersample(df= DF, prop = 2)\n",
    "df.shape\n",
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d31c95",
   "metadata": {},
   "source": [
    "## Oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d35ff30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:22:32.328606Z",
     "start_time": "2022-01-11T21:22:32.326268Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = oversample(df =df , prop = 0.3 , seed = 123)\n",
    "# df.shape\n",
    "# df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ae9f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T20:19:51.670017Z",
     "start_time": "2022-01-11T20:19:51.664240Z"
    }
   },
   "source": [
    "# Data split train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c27ea20a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:24:27.492411Z",
     "start_time": "2022-01-11T21:24:27.445949Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split in X, y\n",
    "X = df.copy() \n",
    "X.drop(columns = \"label\", inplace = True)\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=42)\n",
    "assert(len(X_train)==len(y_train))\n",
    "assert(len(X_test)==len(y_test))\n",
    "assert(len(X_train)+len(X_test)==X.shape[0])\n",
    "\n",
    "# Split X into text and features\n",
    "X_train_text = X_train[[\"text\"]]\n",
    "X_test_text = X_test[[\"text\"]]\n",
    "\n",
    "X_train_features = X_train[[\"score_punctuation\", \"score_capital_word\"]]\n",
    "X_test_features = X_test[[\"score_punctuation\", \"score_capital_word\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9253cf99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:11:24.757485Z",
     "start_time": "2022-01-11T21:11:24.748902Z"
    }
   },
   "source": [
    "# Clean and tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2e64cb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:24:33.359296Z",
     "start_time": "2022-01-11T21:24:28.404329Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(X_text.head())\n",
    "X_train_text = apply_data_cleaning(X = X_train_text, text = \"text\",drop_text = True)#, remove_special_char_2lower_case = True)\n",
    "X_test_text = apply_data_cleaning(X = X_test_text, text = \"text\", drop_text = True)#, remove_special_char_2lower_case = True)\n",
    "\n",
    "X_train_text = list(X_train_text.sentences)\n",
    "y_train = np.array(y)\n",
    "\n",
    "X_test_text = list(X_test_text.sentences)\n",
    "y_test = np.array(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a56f308",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:25:31.541945Z",
     "start_time": "2022-01-11T21:25:31.516103Z"
    }
   },
   "source": [
    "# Embedding TF\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20dd0543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:28:53.026620Z",
     "start_time": "2022-01-11T21:27:51.307777Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(list(api.info()['models'].keys()))\n",
    "ll = ['fasttext-wiki-news-subwords-300',\n",
    " 'conceptnet-numberbatch-17-06-300', \n",
    " 'word2vec-ruscorpora-300', \n",
    " 'word2vec-google-news-300', \n",
    " 'glove-wiki-gigaword-50', \n",
    " 'glove-wiki-gigaword-100',\n",
    " 'glove-wiki-gigaword-200', \n",
    " 'glove-wiki-gigaword-300', \n",
    " 'glove-twitter-25', \n",
    " 'glove-twitter-50', \n",
    " 'glove-twitter-100', \n",
    " 'glove-twitter-200', \n",
    " '__testing_word2vec-matrix-synopsis']\n",
    "word2vec_transfer = api.load(\"glove-twitter-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8bf12a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
