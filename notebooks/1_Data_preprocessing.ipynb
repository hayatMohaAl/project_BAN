{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54bab7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T13:15:11.008706Z",
     "start_time": "2022-01-13T13:15:09.697051Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip show pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d923a3d",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "465bbf60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:49:34.380740Z",
     "start_time": "2022-01-13T20:49:30.399005Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-13 21:49:32.348580: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-13 21:49:32.348603: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import string \n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "# from nltk.corpus import wordnet\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# #from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import spacy\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# from tensorflow.keras import Sequential, layers\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tensorflow.keras import models\n",
    "\n",
    "from sklearn.utils import resample\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd507c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:49:34.386219Z",
     "start_time": "2022-01-13T20:49:34.382509Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d43433",
   "metadata": {},
   "source": [
    "# Help functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b37a6",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c67b12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:49:36.210521Z",
     "start_time": "2022-01-13T20:49:36.202701Z"
    }
   },
   "outputs": [],
   "source": [
    "def undersample(df, prop = 2):\n",
    "    \n",
    "    # get label 2\n",
    "    df2 = df[df[\"label\"]==2]\n",
    "    n2 = df2.shape[0]\n",
    "    \n",
    "    # get label 0\n",
    "    df0= df[df[\"label\"]==0]\n",
    "    df0 = df0.sample(frac=1)\n",
    "    \n",
    "    # sample from label 0\n",
    "    n0 = round(prop*n2)\n",
    "    df0_sample = df0.head(n0)\n",
    "    \n",
    "    # concatenate\n",
    "    df = pd.concat([df0_sample,df2], axis = 0)\n",
    "    df = df.sample(frac=1)\n",
    "    df.reset_index(inplace = True)\n",
    "    \n",
    "    assert(n0==prop*n2)\n",
    "    return df[[\"text\",\"score_punctuation\", \"score_capital_word\", \"label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d508f26",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5297fb70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:49:36.599793Z",
     "start_time": "2022-01-13T20:49:36.593001Z"
    }
   },
   "outputs": [],
   "source": [
    "def oversample(df, prop = 0.3):# , seed = 123):\n",
    "    n = df.shape[0]\n",
    "    n2_new = round(prop*n) \n",
    "    \n",
    "    # get label 2\n",
    "    df2 = df[df[\"label\"]==2]\n",
    "    n2 = df2.shape[0]\n",
    "    n2_resample = n2_new - n2\n",
    "    if (n2_resample) < 0:\n",
    "        print(f\"WARNING: proportion {prop} already satisfied, DF unchanged\")\n",
    "    \n",
    "    else : \n",
    "        indices = list(range(n2))\n",
    "    \n",
    "        # resample from df2\n",
    "        indices_resample = resample(indices, replace=True, n_samples =n2_resample)#, random_state=f\"{seed}\")\n",
    "        df2_resample = df2.iloc[indices_resample]\n",
    "\n",
    "        df2_new = pd.concat([df2_resample, df2], axis =0)\n",
    "\n",
    "        df_new = pd.concat([df2_new, df[df[\"label\"]==0]], axis =0)\n",
    "        df = df_new\n",
    "    #assert(df2_new.shape[0]==prop*n)\n",
    "    df = df.sample(frac=1)\n",
    "    df.reset_index(inplace = True)\n",
    "    return df[[\"text\",\"score_punctuation\", \"score_capital_word\", \"label\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73ea6e",
   "metadata": {},
   "source": [
    "## Data cleaning and tokenizing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3af859ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:49:37.032129Z",
     "start_time": "2022-01-13T20:49:37.023493Z"
    }
   },
   "outputs": [],
   "source": [
    "# # clean each tweet\n",
    "# def clean_data(data, remove_special_char_2lower_case = True):\n",
    "    \n",
    "#     #Removing URLs with a regular expression\n",
    "#     url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "#     data = url_pattern.sub(r'', data)\n",
    "\n",
    "#     # Remove Emails\n",
    "#     data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "   \n",
    "#     # tokenize + remove scpecial characters + set to lower case\n",
    "#     if remove_special_char_2lower_case == True:\n",
    "#         data = text_to_word_sequence(data) \n",
    "#     else :\n",
    "#     # tokenize \n",
    "#         data = data.split() \n",
    "    \n",
    "    \n",
    "#     # Remove stopwords\n",
    "#     stop_words = set(stopwords.words('english')) \n",
    "#     data = [w for w in data if not w in stop_words]         \n",
    "    \n",
    "#     # Remove digits\n",
    "#     data = ' '.join(word for word in data if not word.isdigit())\n",
    "    \n",
    "    \n",
    "#     return text_to_word_sequence(data)\n",
    "\n",
    "# # clean dataset\n",
    "# def apply_data_cleaning(X, text, drop_text = False, remove_special_char_2lower_case = True):\n",
    "#     ln = X.shape[0]\n",
    "#     sentences = []\n",
    "#     for i in range(ln):\n",
    "#         tmp = X.iloc[i][f'{text}']\n",
    "#         tmp_clean = clean_data(tmp,remove_special_char_2lower_case = f\"{remove_special_char_2lower_case}\")\n",
    "#         sentences.append(tmp_clean)\n",
    "#     X[\"sentences\"] = sentences\n",
    "#     if drop_text == True:\n",
    "#         X.drop(columns = f'{text}', inplace = True)\n",
    "#     return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48ad55d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:49:37.585636Z",
     "start_time": "2022-01-13T20:49:37.573112Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \n",
    "    #Removing URLs with a regular expression\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data = url_pattern.sub(r'', data)\n",
    "\n",
    "    # Remove Emails\n",
    "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "   \n",
    "    # tokenize + remove scpecial characters + set to lower case\n",
    "    data = text_to_word_sequence(data) \n",
    "    \n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    data = [w for w in data if not w in stop_words]         \n",
    "    \n",
    "    # Remove digits\n",
    "    data = ' '.join(word for word in data if not word.isdigit())\n",
    "    \n",
    "    \n",
    "    return text_to_word_sequence(data)\n",
    "\n",
    "def apply_data_cleaning(X, text, drop_text = False):\n",
    "    ln = X.shape[0]\n",
    "    sentences = []\n",
    "    for i in range(ln):\n",
    "       # print (f\"{i}\")\n",
    "        tmp = X.iloc[i][f'{text}']\n",
    "        tmp_clean = clean_data(tmp)\n",
    "        sentences.append(tmp_clean)\n",
    "    X[\"sentences\"] = sentences\n",
    "    if drop_text == True:\n",
    "        X.drop(columns = f'{text}', inplace = True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c909fb53",
   "metadata": {},
   "source": [
    "## Help function for Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21cbc1bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:49:38.340723Z",
     "start_time": "2022-01-13T20:49:38.333254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96b88e2",
   "metadata": {},
   "source": [
    "# Help function adding features to embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "476200f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:49:38.842879Z",
     "start_time": "2022-01-13T20:49:38.830374Z"
    }
   },
   "outputs": [],
   "source": [
    "def append_features_to_tensor(X_text, X_feature, _max ):\n",
    "\n",
    "    tmp = np.empty(shape=(X_text.shape[0],X_text.shape[1]+2, X_text.shape[2] ))\n",
    "    tmp[:,0:X_text.shape[1],:] = X_text\n",
    "\n",
    "    first_indices = X_text.shape[0] ## # of twitts \n",
    "    tmp_punct =  np.zeros(shape = (1,X_text.shape[2]))\n",
    "    tmp_cap = np.zeros(shape = (1,X_text.shape[2]))\n",
    "\n",
    "    for i in range(first_indices):\n",
    "        if X_feature.iloc[i,0] == 1:\n",
    "            tmp_punct =  tmp_punct*(_max+0.5)\n",
    "\n",
    "        if X_feature.iloc[i,1] == 1:\n",
    "            tmp_cap = tmp_cap*(_max+1)   \n",
    "\n",
    "        tmp[i,200,:] = tmp_punct\n",
    "        tmp[i,201,:] = tmp_cap\n",
    "\n",
    "    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba5b895",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63050ed",
   "metadata": {},
   "source": [
    "## Load re-labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "852722db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:49:40.769431Z",
     "start_time": "2022-01-13T20:49:40.286545Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../raw_data/DF', 'rb') as handle:\n",
    "    DF = pickle.load(handle)\n",
    "DF.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6946bb7",
   "metadata": {},
   "source": [
    "## Undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e6cea58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:49:42.350288Z",
     "start_time": "2022-01-13T20:49:42.157549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    90819\n",
       "2    10091\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = undersample(df= DF, prop = 9)\n",
    "df.shape\n",
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d31c95",
   "metadata": {},
   "source": [
    "## Oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d35ff30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:49:43.459273Z",
     "start_time": "2022-01-13T20:49:43.377370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    90819\n",
       "2    30273\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = oversample(df =df , prop = 0.3 )#, seed = 123)\n",
    "df.shape\n",
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf97f55",
   "metadata": {},
   "source": [
    "## SubSampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67210a4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:50:15.727781Z",
     "start_time": "2022-01-13T20:50:15.719210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7517\n",
       "2    2483\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N =10000\n",
    "df = df.head(N)\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec3d037",
   "metadata": {},
   "source": [
    "## Replace emojis with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b26723c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:50:24.396429Z",
     "start_time": "2022-01-13T20:50:20.266297Z"
    }
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "\n",
    "emoji_replace = []\n",
    "for tweet in df.text:\n",
    "    emoji_replace.append((emoji.demojize(tweet, delimiters=(\" \", \" \"))).replace(\"_\",\"\"))\n",
    "\n",
    "df.text = emoji_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea77626a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:50:45.909402Z",
     "start_time": "2022-01-13T20:50:45.893495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score_punctuation</th>\n",
       "      <th>score_capital_word</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NOT ME. Personal Experience!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BUILD THAT DAMN WALL!  WHAT YOU WAITING FOR? P...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I don’t even know if you will read this but I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EARLY VOTING Is ON...GO VOTE\\n\\nWe've 1 Month ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>That’s always been wild to me. I was able to u...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score_punctuation  \\\n",
       "0                      NOT ME. Personal Experience!                   0   \n",
       "1  BUILD THAT DAMN WALL!  WHAT YOU WAITING FOR? P...                  0   \n",
       "2  I don’t even know if you will read this but I ...                  0   \n",
       "3  EARLY VOTING Is ON...GO VOTE\\n\\nWe've 1 Month ...                  0   \n",
       "4  That’s always been wild to me. I was able to u...                  0   \n",
       "\n",
       "   score_capital_word  label  \n",
       "0                   0      0  \n",
       "1                   1      2  \n",
       "2                   1      2  \n",
       "3                   1      2  \n",
       "4                   1      2  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()\n",
    "#df.to_csv(\"../raw_data/df120k_emoji2text\")\n",
    "# #fonction pr récupérer le nouveau df \n",
    "\n",
    "# path = '' #add the path of the directory where filter_df.pickle is stored \n",
    "# with open(path + 'df2','rb') as read_file:\n",
    "#     df2_noemoji = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ae9f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T20:19:51.670017Z",
     "start_time": "2022-01-11T20:19:51.664240Z"
    }
   },
   "source": [
    "# Data split train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef1d11f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:50:55.036376Z",
     "start_time": "2022-01-13T20:50:55.025603Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split in X, y\n",
    "X = df.copy() \n",
    "X.drop(columns = \"label\", inplace = True)\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=42)\n",
    "assert(len(X_train)==len(y_train))\n",
    "assert(len(X_test)==len(y_test))\n",
    "assert(len(X_train)+len(X_test)==X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bfc4f22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:50:56.880627Z",
     "start_time": "2022-01-13T20:50:56.877717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "8000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(y))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec7399f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:50:57.724911Z",
     "start_time": "2022-01-13T20:50:57.716371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n",
      "(8000, 3)\n",
      "(2000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c27ea20a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:51:00.428527Z",
     "start_time": "2022-01-13T20:51:00.418986Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split X into text and features\n",
    "X_train_text = X_train[[\"text\"]]\n",
    "X_test_text = X_test[[\"text\"]]\n",
    "\n",
    "X_train_features = X_train[[\"score_punctuation\", \"score_capital_word\"]]\n",
    "X_test_features = X_test[[\"score_punctuation\", \"score_capital_word\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dfc9b09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:51:01.276565Z",
     "start_time": "2022-01-13T20:51:01.268563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "2000\n",
      "(8000, 1)\n",
      "(8000, 2)\n",
      "(2000, 1)\n",
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(len(y_test))\n",
    "\n",
    "print(X_train_text.shape)\n",
    "print(X_train_features.shape)\n",
    "\n",
    "print(X_test_text.shape)\n",
    "print(X_test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b84641d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:51:04.506928Z",
     "start_time": "2022-01-13T20:51:04.499849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score_punctuation</th>\n",
       "      <th>score_capital_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9254</th>\n",
       "      <td>Perhaps they’ll thank  when it happens.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>Why people are misusing  hastag when it's been...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>Well, I don't know the details (and neither ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>[new] MeToo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6669</th>\n",
       "      <td>Editors Guild salutes women journalists who ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  score_punctuation  \\\n",
       "9254            Perhaps they’ll thank  when it happens.                  0   \n",
       "1561  Why people are misusing  hastag when it's been...                  0   \n",
       "1670    Well, I don't know the details (and neither ...                  0   \n",
       "6087                                       [new] MeToo                   0   \n",
       "6669  Editors Guild salutes women journalists who ha...                  0   \n",
       "\n",
       "      score_capital_word  \n",
       "9254                   0  \n",
       "1561                   0  \n",
       "1670                   1  \n",
       "6087                   0  \n",
       "6669                   0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8424dcd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:51:05.308583Z",
     "start_time": "2022-01-13T20:51:05.298378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score_punctuation</th>\n",
       "      <th>score_capital_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>The estimates are in:\\n: 45th year, 210,000 at...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>I thought Matt took his entire family and move...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>Reading  stories from  victims are utterly dis...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>\\n \\n\\n \\n \\n \\n  \\n\\n  \\n\\n's  confirms limit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>crossedswords  shield   \\n\\nGOD THE FATHER, T...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  score_punctuation  \\\n",
       "6252  The estimates are in:\\n: 45th year, 210,000 at...                  0   \n",
       "4684  I thought Matt took his entire family and move...                  0   \n",
       "1731  Reading  stories from  victims are utterly dis...                  0   \n",
       "4742  \\n \\n\\n \\n \\n \\n  \\n\\n  \\n\\n's  confirms limit...                  0   \n",
       "4521   crossedswords  shield   \\n\\nGOD THE FATHER, T...                  0   \n",
       "\n",
       "      score_capital_word  \n",
       "6252                   0  \n",
       "4684                   1  \n",
       "1731                   0  \n",
       "4742                   0  \n",
       "4521                   1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9253cf99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:11:24.757485Z",
     "start_time": "2022-01-11T21:11:24.748902Z"
    }
   },
   "source": [
    "# Clean and tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ee2000a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:51:10.536092Z",
     "start_time": "2022-01-13T20:51:08.911499Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(X_text.head())\n",
    "X_train_text = apply_data_cleaning(X = X_train_text, text = \"text\",drop_text = True)#, remove_special_char_2lower_case = True)\n",
    "X_test_text = apply_data_cleaning(X = X_test_text, text = \"text\", drop_text = True)#, remove_special_char_2lower_case = True)\n",
    "\n",
    "X_train_text = list(X_train_text.sentences)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test_text = list(X_test_text.sentences)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "assert(len(X_train_text)==len(y_train))\n",
    "assert(len(X_test_text)==len(y_test))\n",
    "assert(X_train_features.shape[0]==len(y_train))\n",
    "assert(X_test_features.shape[0])==len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb453ffa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:51:11.684943Z",
     "start_time": "2022-01-13T20:51:11.679965Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(len(y_train))\n",
    "# print(len(y_test))\n",
    "\n",
    "# print(len(X_train_text))\n",
    "# print(X_train_features.shape)\n",
    "\n",
    "# print(len(X_test_text))\n",
    "# print(X_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417196f",
   "metadata": {},
   "source": [
    "# Save cleaned and tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa8fb4b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:51:33.234647Z",
     "start_time": "2022-01-13T20:51:33.232509Z"
    }
   },
   "outputs": [],
   "source": [
    "# # X_train\n",
    "# with open('../raw_data/X_train_text', 'wb') as f1:\n",
    "#     pickle.dump(X_train_text, f1)\n",
    "# with open('../raw_data/X_train_features', 'wb') as f1:\n",
    "#     pickle.dump(X_train_features, f1)\n",
    "\n",
    "# # X_test\n",
    "# with open('../raw_data/X_test_text', 'wb') as f1:\n",
    "#     pickle.dump(X_test_text, f1)   \n",
    "# with open('../raw_data/X_test_features', 'wb') as f1:\n",
    "#     pickle.dump(X_test_features, f1)       \n",
    "\n",
    "# # y_train\n",
    "# with open('../raw_data/y_train', 'wb') as f1:\n",
    "#     pickle.dump(y_train, f1)\n",
    "    \n",
    "# # y_test2\n",
    "# with open('../raw_data/y_test', 'wb') as f1:\n",
    "#     pickle.dump(y_test, f1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aaa9dced",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:51:33.843530Z",
     "start_time": "2022-01-13T20:51:33.838256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "8000\n",
      "(8000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(len(X_train_text))\n",
    "print(X_train_features.shape)\n",
    "\n",
    "# print(len(y_test))\n",
    "# print(len(X_test_text))\n",
    "# print(X_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a56f308",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:25:31.541945Z",
     "start_time": "2022-01-11T21:25:31.516103Z"
    }
   },
   "source": [
    "# Embedding TF\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20dd0543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:51:52.761136Z",
     "start_time": "2022-01-13T20:51:52.759132Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(list(api.info()['models'].keys()))\n",
    "ll = ['fasttext-wiki-news-subwords-300',\n",
    " 'conceptnet-numberbatch-17-06-300', \n",
    " 'word2vec-ruscorpora-300', \n",
    " 'word2vec-google-news-300', \n",
    " 'glove-wiki-gigaword-50', \n",
    " 'glove-wiki-gigaword-100',\n",
    " 'glove-wiki-gigaword-200', \n",
    " 'glove-wiki-gigaword-300', \n",
    " 'glove-twitter-25', \n",
    " 'glove-twitter-50', \n",
    " 'glove-twitter-100', \n",
    " 'glove-twitter-200', \n",
    " '__testing_word2vec-matrix-synopsis']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2674a822",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-13T20:55:24.968Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(list(api.info()['models'].keys()))\n",
    "word2vec_transfrer = api.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e8bf12a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T20:54:26.142137Z",
     "start_time": "2022-01-13T20:52:41.691745Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2vec_transfer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_40060/1849139970.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Embed the training and test sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_train_embed_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_transfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX_test_embed_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_transfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word2vec_transfer' is not defined"
     ]
    }
   ],
   "source": [
    "# Embed the training and test sentences\n",
    "X_train_embed_tf = embedding(word2vec_transfer, X_train_text)\n",
    "X_test_embed_tf = embedding(word2vec_transfer, X_test_text)\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad = pad_sequences(X_train_embed_tf, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad = pad_sequences(X_test_embed_tf, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b9e743",
   "metadata": {},
   "source": [
    "## Pickle and save embedded data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778daab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:12.402400Z",
     "start_time": "2022-01-12T14:51:12.395187Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(y_train))\n",
    "print(len(y_test))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b482999",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:12.420853Z",
     "start_time": "2022-01-12T14:51:12.404648Z"
    }
   },
   "outputs": [],
   "source": [
    "# # X_train\n",
    "# with open('../raw_data/X_train_pad', 'wb') as f1:\n",
    "#     pickle.dump(X_train_pad, f1)\n",
    "    \n",
    "# # X_test\n",
    "# with open('../raw_data/X_test_pad', 'wb') as f1:\n",
    "#     pickle.dump(X_test_pad, f1)   \n",
    "    \n",
    "\n",
    "# # y_train\n",
    "# with open('../raw_data/y_train', 'wb') as f1:\n",
    "#     pickle.dump(y_train, f1)\n",
    "    \n",
    "# # y_test2\n",
    "# with open('../raw_data/y_test', 'wb') as f1:\n",
    "#     pickle.dump(y_test, f1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c3d0bf",
   "metadata": {},
   "source": [
    "# Add features to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da415f26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:12.513268Z",
     "start_time": "2022-01-12T14:51:12.504311Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pad_added_features = append_features_to_tensor(X_train_pad, \\\n",
    "                                                       X_feature = X_train_features, \\\n",
    "                                                       _max = X_train_pad.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c8f2bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:12.518953Z",
     "start_time": "2022-01-12T14:51:12.514523Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_pad_added_features = append_features_to_tensor(X_test_pad, \\\n",
    "                                                       X_feature = X_test_features, \\\n",
    "                                                       _max = X_test_pad.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61000c3b",
   "metadata": {},
   "source": [
    "# Pickle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a89a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cba5fc1",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b00d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:12.733464Z",
     "start_time": "2022-01-12T14:51:12.521345Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Masking())#, label =\"layer1_masking\")\n",
    "    model.add(layers.LSTM(20, activation='tanh'))#, label =\"layer2_LSTM\")\n",
    "    model.add(layers.Dense(15, activation='relu'))#, label =\"layer3_dense_relu\")\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))#, label =\"layer4_dense_sigmoid\")\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=[\"Recall\"]) #['recall']) # tp/(tp+fn) metrics=[tf.keras.metrics.Recall()]\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = init_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a924c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:19.233118Z",
     "start_time": "2022-01-12T14:51:12.735028Z"
    }
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "#%%time\n",
    "history = model.fit(X_train_pad, y_train, \n",
    "          batch_size = 32,  # low --> optimizes on smaller ss --> faster --> but to generalize \n",
    "          epochs=10,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed36f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:19.242881Z",
     "start_time": "2022-01-12T14:51:19.234952Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss_recall(history, title=None):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "    \n",
    "    # --- LOSS --- \n",
    "    \n",
    "    ax[0].plot(history.history['loss'])\n",
    "    ax[0].plot(history.history['val_loss'])\n",
    "    ax[0].set_title('Model loss')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylim((0,3))\n",
    "    ax[0].legend(['Train', 'Test'], loc='best')\n",
    "    ax[0].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[0].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    # --- ACCURACY\n",
    "    \n",
    "    ax[1].plot(history.history['recall'])\n",
    "    ax[1].plot(history.history['val_recall'])\n",
    "    ax[1].set_title('Model Recall')\n",
    "    ax[1].set_ylabel('recall')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Test'], loc='best')\n",
    "    ax[1].set_ylim((0,1))\n",
    "    ax[1].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[1].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    if title:\n",
    "        fig.suptitle(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a5ddc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:19.718879Z",
     "start_time": "2022-01-12T14:51:19.244561Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss_recall(model.history, title=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b808d",
   "metadata": {},
   "source": [
    "## Select epoch and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fc62cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:51:19.729023Z",
     "start_time": "2022-01-12T14:51:19.723503Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# filepath = './saved_model'\n",
    "# save_model(model, filepath)\n",
    "# model = load_model(filepath, compile = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847839fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:52:01.914158Z",
     "start_time": "2022-01-12T14:51:56.574994Z"
    }
   },
   "outputs": [],
   "source": [
    "model = init_model()\n",
    "model.fit(X_train_pad, y_train, \n",
    "          batch_size = 32,  # low --> optimizes on smaller ss --> faster --> but to generalize \n",
    "          epochs=10,\n",
    "          validation_split=0.3#,\n",
    "        #  callbacks=[es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7915077b",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f06f30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:54:00.494405Z",
     "start_time": "2022-01-12T14:54:00.330461Z"
    }
   },
   "outputs": [],
   "source": [
    "! ls ../project_BAN/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdeb6c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:54:07.608981Z",
     "start_time": "2022-01-12T14:54:03.917125Z"
    }
   },
   "outputs": [],
   "source": [
    "models.save_model(model, '../project_BAN/models/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1a7164",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1959a0df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T15:01:09.442064Z",
     "start_time": "2022-01-12T15:01:07.283254Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded_model = models.load_model('../project_BAN/models/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136fd14f",
   "metadata": {},
   "source": [
    "# Predict from tweet - test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7a0c8",
   "metadata": {},
   "source": [
    "### Prediction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cd37d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e4b67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:44:45.115240Z",
     "start_time": "2022-01-12T14:44:45.083407Z"
    }
   },
   "outputs": [],
   "source": [
    "# write tweet\n",
    "tweet = \"I love deep learning\"\n",
    "\n",
    "score_punctuation = ponctuation_check(tweet)\n",
    "score_capitalization = capital_word_check(tweet)\n",
    "\n",
    "tweet_tokenized = clean_data(tweet)\n",
    "tweet_tokenized\n",
    "\n",
    "tweet_emebedded = embed_sentence_with_TF(word2vec_transfer, tweet_tokenized)\n",
    "tweet_emebedded_reshape  = tweet_emebedded.reshape((1,tweet_emebedded.shape[0], tweet_emebedded.shape[1]))\n",
    "\n",
    "res = loaded_model.predict(tweet_reshape)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62662bf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:44:36.946229Z",
     "start_time": "2022-01-12T14:44:36.946209Z"
    }
   },
   "outputs": [],
   "source": [
    "if res[0][0]< 0.5:\n",
    "    print(\"good tweet\")\n",
    "else:\n",
    "    print(\"hate tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73f6e44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T08:19:48.084533Z",
     "start_time": "2022-01-13T08:19:00.680428Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import pickle\n",
    "word2vec_transfimporer = api.load('glove-twitter-50')\n",
    "with open('../project_BAN/models/word2vec_transfer', 'wb') as f1:\n",
    "    pickle.dump(word2vec_transfer, f1, protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f033fcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T08:20:25.125233Z",
     "start_time": "2022-01-13T08:20:23.088112Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../project_BAN/models/word2vec_transfer', 'wb') as f1:\n",
    "    pickle.dump(word2vec_transfer, f1, protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f5521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../project_BAN/models/word2vec_transfer', 'rb') as handle:\n",
    "   #     word2vec_transfer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fce283",
   "metadata": {},
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c1d642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T08:59:14.079362Z",
     "start_time": "2022-01-13T08:59:14.070557Z"
    }
   },
   "source": [
    "- Replace emojis by text in traning data\n",
    "\n",
    "- Prep data upto embedding and pickle  \n",
    "\n",
    "------ new jn ----------------\n",
    "- import pickled data + add features\n",
    "- save as pickle\n",
    "\n",
    "------------ new jn ------------------\n",
    "run models \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba6bf92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "625.455px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
