{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a50088",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c07d360f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/vruccio/.pyenv/versions/3.8.12/envs/project_BAN/lib/python3.8/site-packages (3.6.7)\n",
      "Requirement already satisfied: tqdm in /home/vruccio/.pyenv/versions/3.8.12/envs/project_BAN/lib/python3.8/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: joblib in /home/vruccio/.pyenv/versions/3.8.12/envs/project_BAN/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/vruccio/.pyenv/versions/3.8.12/envs/project_BAN/lib/python3.8/site-packages (from nltk) (2021.11.10)\n",
      "Requirement already satisfied: click in /home/vruccio/.pyenv/versions/3.8.12/envs/project_BAN/lib/python3.8/site-packages (from nltk) (8.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f8ac0ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.1.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/vruccio/.pyenv/versions/3.8.12/envs/project_BAN/lib/python3.8/site-packages (from gensim) (1.22.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/vruccio/.pyenv/versions/3.8.12/envs/project_BAN/lib/python3.8/site-packages (from gensim) (1.7.3)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.1.2 smart-open-5.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fbd6d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.7.0-cp38-cp38-manylinux2010_x86_64.whl (489.6 MB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 489.6 MB 302 bytes/s          \n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.43.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.1 MB 3.7 MB/s            \n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.21.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.23.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.1 MB 3.4 MB/s            \n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.19.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1 MB 4.3 MB/s            \n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.6.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.5 MB 3.7 MB/s            \n",
      "\u001b[?25hCollecting keras<2.8,>=2.7.0rc0\n",
      "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3 MB 2.6 MB/s            \n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/vruccio/.pyenv/versions/3.8.12/envs/project_BAN/lib/python3.8/site-packages (from tensorflow) (1.22.0)\n",
      "Collecting wheel<1.0,>=0.32.0\n",
      "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-12.0.0-2-py2.py3-none-manylinux1_x86_64.whl (13.3 MB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.3 MB 4.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /home/vruccio/.pyenv/versions/3.8.12/envs/project_BAN/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorboard~=2.6\n",
      "  Using cached tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting flatbuffers<3.0,>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/vruccio/.pyenv/versions/3.8.12/envs/project_BAN/lib/python3.8/site-packages (from tensorflow) (4.0.1)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126 kB 3.0 MB/s            \n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast<0.5.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
      "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 463 kB 3.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /home/vruccio/.pyenv/versions/3.8.12/envs/project_BAN/lib/python3.8/site-packages (from tensorflow) (1.13.3)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155 kB 4.6 MB/s            \n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 781 kB 3.7 MB/s            \n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/vruccio/.pyenv/versions/3.8.12/envs/project_BAN/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/vruccio/.pyenv/versions/3.8.12/envs/project_BAN/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (56.0.0)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97 kB 2.2 MB/s            \n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.10.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vruccio/.pyenv/versions/3.8.12/envs/project_BAN/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/vruccio/.pyenv/versions/3.8.12/envs/project_BAN/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/vruccio/.pyenv/versions/3.8.12/envs/project_BAN/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vruccio/.pyenv/versions/3.8.12/envs/project_BAN/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/vruccio/.pyenv/versions/3.8.12/envs/project_BAN/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.7.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Using legacy 'setup.py install' for termcolor, since package 'wheel' is not installed.\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, wheel, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "    Running setup.py install for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-4.2.4 flatbuffers-2.0 gast-0.4.0 google-auth-2.3.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.43.0 h5py-3.6.0 importlib-metadata-4.10.0 keras-2.7.0 keras-preprocessing-1.1.2 libclang-12.0.0 markdown-3.3.6 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.19.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.8 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.23.1 termcolor-1.1.0 werkzeug-2.0.2 wheel-0.37.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6ec3b86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:04.886305Z",
     "start_time": "2022-01-07T15:23:59.189919Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6664/603901381.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext_to_word_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import string \n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "#from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.corpus import wordnet\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# #from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#import spacy\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print('Done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38fd5ba",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99708015",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:26:06.739721Z",
     "start_time": "2022-01-07T15:26:03.662695Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', None) \n",
    "df = pd.read_csv(\"~/Downloads/banData/MeTooHate.csv\")[[\"text\", \"category\"]]\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a9feed",
   "metadata": {},
   "source": [
    "## Undersample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a6b4299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:26:09.560982Z",
     "start_time": "2022-01-07T15:26:09.065746Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = df[df[\"category\"]== 1]\n",
    "df1 = df[df[\"category\"]== 1].dropna()\n",
    "df0 = df[df[\"category\"]== 0].dropna()\n",
    "df = pd.concat([df0.sample(df1.shape[0]), df1], axis = 0)\n",
    "\n",
    "\n",
    "#shuffle rows\n",
    "df = df.sample(frac=1)\n",
    "df.shape\n",
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94999ad7",
   "metadata": {},
   "source": [
    "## Select sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e49f49e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:26:11.243365Z",
     "start_time": "2022-01-07T15:26:11.233153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    56\n",
       "0    44\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 100\n",
    "df = df.head(N)\n",
    "df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d571bb3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:26:12.448635Z",
     "start_time": "2022-01-07T15:26:12.442902Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df[\"category\"]\n",
    "X = df.drop(columns = [\"category\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ed67c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T15:49:20.415916Z",
     "start_time": "2022-01-06T15:49:20.408766Z"
    }
   },
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7d87924",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:26:20.744435Z",
     "start_time": "2022-01-07T15:26:20.733457Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \n",
    "    #Removing URLs with a regular expression\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data = url_pattern.sub(r'', data)\n",
    "\n",
    "    # Remove Emails\n",
    "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "   \n",
    "    # tokenize + remove scpecial characters + set to lower case\n",
    "    data = text_to_word_sequence(data) \n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    data = [w for w in data if not w in stop_words]         \n",
    "    \n",
    "    # Remove digits\n",
    "    data = ' '.join(word for word in data if not word.isdigit())\n",
    "    \n",
    "    \n",
    "    return text_to_word_sequence(data)\n",
    "\n",
    "def apply_data_cleaning(X, text, drop_text = False):\n",
    "    ln = X.shape[0]\n",
    "    sentences = []\n",
    "    for i in range(ln):\n",
    "        tmp = X.iloc[i][f'{text}']\n",
    "        tmp_clean = clean_data(tmp)\n",
    "        sentences.append(tmp_clean)\n",
    "    X[\"sentences\"] = sentences\n",
    "    if drop_text == True:\n",
    "        X.drop(columns = f'{text}', inplace = True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccb77746",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:26:24.622081Z",
     "start_time": "2022-01-07T15:26:24.580895Z"
    }
   },
   "outputs": [],
   "source": [
    "X = apply_data_cleaning(X = X, text = \"text\", drop_text = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0667df0",
   "metadata": {},
   "source": [
    "## Train-Test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "772be889",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:26:34.898659Z",
     "start_time": "2022-01-07T15:26:34.889240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "80\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "X = list(X.sentences)\n",
    "y = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=42)\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e55147",
   "metadata": {},
   "source": [
    "## Tokenize  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a487c622",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.331462Z",
     "start_time": "2022-01-07T15:24:09.331444Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializes a Keras utilities that does all the tokenization for you\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# The tokenization learns a dictionnary that maps a token (integer) to each word\n",
    "# It can be done only on the train set - we are not supposed to know the test set !\n",
    "# This tokenization also lower your words, apply some filters, and so on - you can check the doc if you want\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "    \n",
    "# We apply the tokenization to the train and test set\n",
    "X_train_token = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_token = tokenizer.texts_to_sequences(X_test)\n",
    "#X_test_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc726231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.335945Z",
     "start_time": "2022-01-07T15:24:09.335922Z"
    }
   },
   "outputs": [],
   "source": [
    "## sjekk\n",
    "# sentence_number = 10\n",
    "\n",
    "# input_raw = X_train[sentence_number]\n",
    "# input_token = X_train_token[sentence_number]\n",
    "# for i in range(2):\n",
    "#     print(f'Word : {input_raw[i]} -> Token {input_token[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5567244b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T21:27:42.019998Z",
     "start_time": "2022-01-06T21:27:42.013444Z"
    }
   },
   "source": [
    "### Add vocabulary\n",
    "\n",
    "The dictionary that maps each word to a token can be accessed with `tokenizer.word_index`\n",
    "    \n",
    "Add a `vocab_size` variable that stores the number of different words (=tokens) in the train set. This is called the _size of the vocabulary_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e493d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.337101Z",
     "start_time": "2022-01-07T15:24:09.337090Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "print(f'There are {vocab_size} different words in the train set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec81718",
   "metadata": {},
   "source": [
    "## Padding\n",
    "_filling_ cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d39c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.338075Z",
     "start_time": "2022-01-07T15:24:09.338063Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train_token, dtype='float32', padding='post')\n",
    "X_test_pad = pad_sequences(X_test_token, dtype='float32', padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7609ec",
   "metadata": {},
   "source": [
    "!set maxlen <<< lower than longest sentence for efficiency/wt loss info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47617d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.344885Z",
     "start_time": "2022-01-07T15:24:09.344872Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_hist(X):\n",
    "    len_ = [len(_) for _ in X]\n",
    "    plt.hist(len_)\n",
    "    plt.title('Histogram of the number of sentences that have a given number of words')\n",
    "    plt.show()\n",
    "    \n",
    "plot_hist(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4403520a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.346414Z",
     "start_time": "2022-01-07T15:24:09.346398Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train_token, dtype='float32', padding='post', maxlen=20)\n",
    "X_test_pad = pad_sequences(X_test_token, dtype='float32', padding='post', maxlen=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1048814e",
   "metadata": {},
   "source": [
    "# RNN - embedding trained on corpus\n",
    "\n",
    "Let's now feed this data to a Recurrent Neural Network.\n",
    "\n",
    "model:\n",
    "- an embedding layer whose `input_dim` is the size of your vocabulary (= your `vocab_size`), and whose `output_dim` is the size of the embedding space you want to have\n",
    "- a RNN (SimpleRNN, LSTM, GRU) layer\n",
    "- a Dense layer\n",
    "- an output layer\n",
    "\n",
    "âš ï¸ **Warning** âš ï¸ Here, you don't need a masking layer. Why? Because `layers.Embedding` has a argument to do that directly, which you have to set with `mask_zero=True`. That also means that your data **HAVE TO** be padded with **0** (which is the default behavior). See the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding#example_2) to understand how it **impacts** the `input_dim`.\n",
    "\n",
    "!`input_dim` should equal size of vocabulary + 1\n",
    "\n",
    "\n",
    "Compile it with the appropriate arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff38339",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.347068Z",
     "start_time": "2022-01-07T15:24:09.347054Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size + 1, output_dim=embedding_dimension, mask_zero=True))\n",
    "model.add(layers.LSTM(20))\n",
    "model.add(layers.Dense(10, activation=\"relu\"))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c763cae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.352505Z",
     "start_time": "2022-01-07T15:24:09.352485Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d6f80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.353252Z",
     "start_time": "2022-01-07T15:24:09.353242Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(f'Expected number of parameters : {(vocab_size + 1) * embedding_dimension}')\n",
    "es = EarlyStopping(patience=4, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_pad, y_train, \n",
    "          epochs=20, \n",
    "          batch_size=16,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4576a3af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.353884Z",
     "start_time": "2022-01-07T15:24:09.353873Z"
    }
   },
   "outputs": [],
   "source": [
    "res = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e7b515",
   "metadata": {},
   "source": [
    "# NN Emebdding with Word2Vec - understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49205cf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.354417Z",
     "start_time": "2022-01-07T15:24:09.354409Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6664/2049436110.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train.head[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627d0759",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.354958Z",
     "start_time": "2022-01-07T15:24:09.354948Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences=X_train)\n",
    "wv = word2vec.wv\n",
    "#wv[\"familiar\"] --> in X_train but not wv??\n",
    "size = len(wv['harassment'])\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfabe6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.355642Z",
     "start_time": "2022-01-07T15:24:09.355625Z"
    }
   },
   "outputs": [],
   "source": [
    "wv.most_similar('harassment')\n",
    "#word_embedding = wv[''harassment']\n",
    "#wv.similar_by_vector(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef644e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.363152Z",
     "start_time": "2022-01-07T15:24:09.363090Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Vocabulary size', len(wv.key_to_index))\n",
    "diff_words = set([_ for elt in X_train for _ in elt])\n",
    "print('Number of different words in the train set', len(diff_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ffdb74",
   "metadata": {},
   "source": [
    "In a nutshell, this internal NN predicts a word from the surroundings words in a sentences. So it chooses many splits in the different sentences, choose some words as inputs  ð‘‹  and a word as output  ð‘¦  which it tries to predict, in the embedding space.\n",
    "\n",
    "And as any neural network, Word2Vec has some hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d61b06",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Word2Vec hyperparameters\n",
    "\n",
    "\n",
    "The first important hyperparameter is the `vector_size` argument. It corresponds to the size of the embedding space. Learn a new `word2vec_2` model, still trained on the `X_train`, but with a smaller or higher `vector_size`.\n",
    "\n",
    "Verify on some words that the corresponding embedding is of your selected size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48a7b8",
   "metadata": {},
   "source": [
    "## vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dae46c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.365451Z",
     "start_time": "2022-01-07T15:24:09.365429Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_size30 = Word2Vec(sentences=X_train, vector_size = 30)\n",
    "wv_size30 = word2vec_size30.wv\n",
    "wv_size30.most_similar('harassment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee473f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.368903Z",
     "start_time": "2022-01-07T15:24:09.368859Z"
    }
   },
   "outputs": [],
   "source": [
    "# print('Vocabulary size', len(wv.key_to_index))\n",
    "# diff_words = set([_ for elt in X_train for _ in elt])\n",
    "# print('Number of different words in the train set', len(diff_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b6555",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.370359Z",
     "start_time": "2022-01-07T15:24:09.370338Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_size50 = Word2Vec(sentences=X_train, vector_size = 50)\n",
    "wv_size50 = word2vec_size50.wv\n",
    "wv_size50.most_similar('harassment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7a477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.371121Z",
     "start_time": "2022-01-07T15:24:09.371109Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_size100 = Word2Vec(sentences=X_train, vector_size = 100)\n",
    "wv_size100 = word2vec_size100.wv\n",
    "wv_size100.most_similar('harassment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281fcbe",
   "metadata": {},
   "source": [
    "## min_count\n",
    "Second hyperparameter `min_count`,  min # of times a word has to occur in order to be in embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2889445f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.372372Z",
     "start_time": "2022-01-07T15:24:09.372357Z"
    }
   },
   "outputs": [],
   "source": [
    "# min_count:  min # of times a word has to occur in order to nbe in embedding space\n",
    "word2vec3 = Word2Vec(sentences=X_train, vector_size = 30, min_count = 1)\n",
    "wv3 = word2vec.wv\n",
    "wv3.most_similar('harassment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b50e8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.373345Z",
     "start_time": "2022-01-07T15:24:09.373329Z"
    }
   },
   "outputs": [],
   "source": [
    "## wv3[\"familiar\"] -- > syill not in wv, why?\n",
    "print('Vocabulary size', len(wv3.key_to_index))\n",
    "diff_words = set([_ for elt in X_train for _ in elt])\n",
    "print('Number of different words in the train set', len(diff_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa35aa73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.374624Z",
     "start_time": "2022-01-07T15:24:09.374600Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_0 = Word2Vec(sentences=X_train, vector_size=50, min_count=1)\n",
    "word2vec_1 = Word2Vec(sentences=X_train, vector_size=50, min_count=2)\n",
    "word2vec_2 = Word2Vec(sentences=X_train, vector_size=50, min_count=3)\n",
    "word2vec_3 = Word2Vec(sentences=X_train, vector_size=50, min_count=5)\n",
    "word2vec_4 = Word2Vec(sentences=X_train, vector_size=50, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec180f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.377798Z",
     "start_time": "2022-01-07T15:24:09.377777Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_1.wv[\"harassment\"]\n",
    "len(word2vec_1.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9476f6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.380381Z",
     "start_time": "2022-01-07T15:24:09.380342Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Number of word in W2V #1 : {len(word2vec_0.wv.key_to_index)}')\n",
    "print(f'Number of word in W2V #1 : {len(word2vec_1.wv.key_to_index)}')\n",
    "print(f'Number of word in W2V #2 : {len(word2vec_2.wv.key_to_index)}')\n",
    "print(f'Number of word in W2V #3 : {len(word2vec_3.wv.key_to_index)}')\n",
    "print(f'Number of word in W2V #4 : {len(word2vec_4.wv.key_to_index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c1975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.382584Z",
     "start_time": "2022-01-07T15:24:09.382569Z"
    }
   },
   "outputs": [],
   "source": [
    "diff_words = set([_ for elt in X_train for _ in elt])\n",
    "len(diff_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9746309",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.383810Z",
     "start_time": "2022-01-07T15:24:09.383796Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_3.wv.most_similar(\"harassment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cef0111",
   "metadata": {},
   "source": [
    "## window\n",
    "The arguments you have seen (vector_size, min_count and window) are usually the one that you should start changing to get a better performance for your model.\n",
    "\n",
    "Other hyperparameters in doc (cf 8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa253fd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.384624Z",
     "start_time": "2022-01-07T15:24:09.384614Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_3_window1 = Word2Vec(sentences=X_train, vector_size=50, min_count=3, window = 1)\n",
    "word2vec_3 = Word2Vec(sentences=X_train, vector_size=50, min_count=3, window = 5) # default 5\n",
    "word2vec_3_window8 = Word2Vec(sentences=X_train, vector_size=50, min_count=3, window = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c22549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.385465Z",
     "start_time": "2022-01-07T15:24:09.385454Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(f'Number of word in W2V #window 1 : {len(word2vec_3_window1 .wv.key_to_index)}')\n",
    "# print(f'Number of word in W2V #window 5: {len(word2vec_3.wv.key_to_index)}')\n",
    "# print(f'Number of word in W2V #window 8 : {len(word2vec_3_window8 .wv.key_to_index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd95faa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.386115Z",
     "start_time": "2022-01-07T15:24:09.386105Z"
    }
   },
   "outputs": [],
   "source": [
    "#word2vec_3_window1.wv.most_similar(\"harassment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a0359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.386741Z",
     "start_time": "2022-01-07T15:24:09.386731Z"
    }
   },
   "outputs": [],
   "source": [
    "#word2vec_3.wv.most_similar(\"harassment\") # default 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c22358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.387373Z",
     "start_time": "2022-01-07T15:24:09.387362Z"
    }
   },
   "outputs": [],
   "source": [
    "#word2vec_3_window8.wv.most_similar(\"harassment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554066d3",
   "metadata": {},
   "source": [
    "# Embedding: convert W2V\n",
    "\n",
    "Remember that word2vec is the first step to the overall process of feeding such a representation into a RNN, as shown here :\n",
    "\n",
    "<img src=\"word2vec_representation.png\" width=\"400px\" />\n",
    "\n",
    "\n",
    "\n",
    "Now, let's work on Step 2 by converting the training and test data into their vector representation to be ready to be feed in RNNs.\n",
    "\n",
    "â“ **Question** â“ Now, write a function that, given a sentence, returns a matrix that corresponds to the embedding of the full sentence, which means that you have to embed each word one after the other and concatenate the result to output a 2D matrix (be sure that your output is a NumPy array)\n",
    "\n",
    "â— **Remark** â— You will probably notice that some words you are trying to convert throw errors as they are said not to belong to the dictionary:\n",
    "\n",
    "- for the test set, this is understandable: some words were not in the train set and thus their embedded representation is unknown\n",
    "- for the train set, due to `min_count` hyperparameter, not all the words have a vector representation\n",
    "\n",
    "In any case, just skip the missing words here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac87bc1c",
   "metadata": {},
   "source": [
    "## Embedding function\n",
    "\n",
    "- each word --> vector w2v\n",
    "- each sentence ---> matrix of w2v \n",
    "- e.g sentence of 5 words and embedding size 10 ---> matrix  5x10 --> # of words>=min_count x vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c686fb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.388059Z",
     "start_time": "2022-01-07T15:24:09.388046Z"
    }
   },
   "outputs": [],
   "source": [
    "def embed_sentence(word2vec, sentence):\n",
    "    \"\"\" \n",
    "    sentence to matrix\n",
    "    \"\"\"\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "   \n",
    "\n",
    "def embedding(word2vec, sentences):\n",
    "    \"\"\" \n",
    "    list(sentences in words)---> list(matrices)\n",
    "    \"\"\"\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a17ae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.390098Z",
     "start_time": "2022-01-07T15:24:09.390073Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = X_train[0]\n",
    "word2vec = Word2Vec(sentences = X_train, vector_size=50, min_count = 2, window = 6)\n",
    "\n",
    "print(embed_sentence(word2vec, sentence).shape)\n",
    "#embed_sentence(word2vec, sentence)  ## --> 4 words in 50D --> 4x50 mx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5111013c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.392146Z",
     "start_time": "2022-01-07T15:24:09.392110Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_embedded = embedding(word2vec, X_train)\n",
    "print(type(X_train))\n",
    "print(len(X_train), len(X_train_embedded))\n",
    "print(f'dim of first sentence  {X_train_embedded[0].shape}')\n",
    "print(f'dim of 2nd  sentence  {X_train_embedded[1].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c21aa2d",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b2f68",
   "metadata": {},
   "source": [
    "In order to have ready-to-use data, do not forget to pad them in order to have tensors that can be divided in batch sizes during the optimization. Store the padedd values in X_train_pad and X_test_pad. Do not forget the important arguments of the padding ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eea7ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T09:31:54.844203Z",
     "start_time": "2022-01-07T09:31:54.837194Z"
    }
   },
   "source": [
    "#! padding with zeros and in the end ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac41978",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.393572Z",
     "start_time": "2022-01-07T15:24:09.393561Z"
    }
   },
   "outputs": [],
   "source": [
    "# embedd\n",
    "X_train_embedded = embedding(word2vec, X_train)\n",
    "X_test_embedded = embedding(word2vec, X_train)\n",
    "\n",
    "assert(len(X_train_embedded) == len(X_train))\n",
    "len(X_train_embedded) # list of 80 matrices of xxx rows and 50 columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ce74e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.394314Z",
     "start_time": "2022-01-07T15:24:09.394301Z"
    }
   },
   "outputs": [],
   "source": [
    "# pad\n",
    "X_train_pad = pad_sequences(X_train_embedded, dtype='float32', padding='post')\n",
    "X_test_pad = pad_sequences(X_test_embedded, dtype='float32', padding='post')\n",
    "\n",
    "assert(len(X_train_pad.shape) == 3)\n",
    "X_train_pad.shape  ## array 80 matrices of 16x50 or 50x16? \n",
    "#!16 lmax length of senetnces in X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae7333",
   "metadata": {},
   "source": [
    "# Modelling NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d154fa83",
   "metadata": {},
   "source": [
    "## Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff9372e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.396548Z",
     "start_time": "2022-01-07T15:24:09.396524Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde18c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.397475Z",
     "start_time": "2022-01-07T15:24:09.397463Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910104c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.398627Z",
     "start_time": "2022-01-07T15:24:09.398617Z"
    }
   },
   "outputs": [],
   "source": [
    "# Embed the training and test sentences\n",
    "X_train_embed = embedding(word2vec, X_train)\n",
    "X_test_embed = embedding(word2vec, X_test)\n",
    "\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=200)\n",
    "\n",
    "\n",
    "for X in [X_train_pad, X_test_pad]:\n",
    "    assert type(X) == np.ndarray\n",
    "    assert X.shape[-1] == word2vec.wv.vector_size\n",
    "\n",
    "\n",
    "assert X_train_pad.shape[0] == len(X_train)\n",
    "assert X_test_pad.shape[0] == len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee1b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f416050",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4445b0fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.399835Z",
     "start_time": "2022-01-07T15:24:09.399813Z"
    }
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "counts = dict(zip(unique, counts))\n",
    "print('Number of labels in train set', counts)\n",
    "print('Baseline accuracy: ', {1/2})\n",
    "print('Acurracy to bit: ', {(4866 + 64190) / 74712})\n",
    "#y_pred = 0 if counts[0] > counts[1] else 1\n",
    "#print('Baseline accuracy: ', accuracy_score(y_test, [y_pred]*len(y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19134b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T09:45:59.808066Z",
     "start_time": "2022-01-07T09:45:59.804711Z"
    }
   },
   "source": [
    "## Initial RNN model \n",
    "use the NN model, and train with my data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e24e5152",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.401126Z",
     "start_time": "2022-01-07T15:24:09.401110Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6664/607931324.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6664/607931324.py\u001b[0m in \u001b[0;36minit_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMasking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "def init_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Masking())\n",
    "    model.add(layers.LSTM(20, activation='tanh'))\n",
    "    model.add(layers.Dense(15, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['Recall'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4bd5ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.402162Z",
     "start_time": "2022-01-07T15:24:09.402148Z"
    }
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_pad, y_train, \n",
    "          batch_size = 32,  # low slower?\n",
    "          epochs=100,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )\n",
    "\n",
    "## Recall Ã  checker > Ã  Kaggle Recall score: 0.548914858096828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48670d57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.403157Z",
     "start_time": "2022-01-07T15:24:09.403142Z"
    }
   },
   "outputs": [],
   "source": [
    "res = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e0e9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.404077Z",
     "start_time": "2022-01-07T15:24:09.404063Z"
    }
   },
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac809e6f",
   "metadata": {},
   "source": [
    "## Improve RNN with transfer learning\n",
    "use embedding done on other corpus \n",
    "Use trained NN with others data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11831377",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:36:11.725635Z",
     "start_time": "2022-01-07T15:36:11.551122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "## list existing models (trained on >>>> data)\n",
    "print(list(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcef137",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.406150Z",
     "start_time": "2022-01-07T15:24:09.406136Z"
    }
   },
   "outputs": [],
   "source": [
    "# pick glove-wiki-gigaword-50 for now, will try others--> teachers recommendation?\n",
    "word2vec_transfer = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad3d83e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.406988Z",
     "start_time": "2022-01-07T15:24:09.406975Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(word2vec_transfer.key_to_index))\n",
    "print(len(word2vec_transfer['harassment']))\n",
    "word2vec_transfer['harassment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7989d5b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.407942Z",
     "start_time": "2022-01-07T15:24:09.407926Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_transfer.most_similar('harrassment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f03248",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.410655Z",
     "start_time": "2022-01-07T15:24:09.410629Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed_2 = embedding(word2vec_transfer, X_train)\n",
    "X_test_embed_2 = embedding(word2vec_transfer, X_test)\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad_2 = pad_sequences(X_train_embed_2, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad_2 = pad_sequences(X_test_embed_2, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d95c52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.412019Z",
     "start_time": "2022-01-07T15:24:09.412005Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run new model\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model = init_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f31a83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.413053Z",
     "start_time": "2022-01-07T15:24:09.413040Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_pad_2, y_train, \n",
    "          batch_size = 32,\n",
    "          epochs=100,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )\n",
    "## A dÃ©passer > Recall score: 0.548914858096828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94752d2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.413817Z",
     "start_time": "2022-01-07T15:24:09.413804Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a491a3c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.414594Z",
     "start_time": "2022-01-07T15:24:09.414581Z"
    }
   },
   "outputs": [],
   "source": [
    "res = model.evaluate(X_test_pad_2, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0a1368",
   "metadata": {},
   "source": [
    "# Question to teachers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11859b09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T16:24:56.252728Z",
     "start_time": "2022-01-06T16:24:56.212677Z"
    }
   },
   "source": [
    "- Lementazing needed? yes--> reduce vocabulary_size in X_train\n",
    "- Removing stop_words ---> yes.\n",
    "- Why words in my X_train do not have embeddings with Word2Vec  --> depends on min_count\n",
    "- keep (top 20) emojis by replacing with some word  instead of removing in data prep? \n",
    "- should we remove numbers in words? ex. \"one\"\n",
    "\n",
    "- vector_size effect on embedding, not clear?---> seems like higher dimension better on most_similar words\n",
    "-! transfer embedding vs \"regular\" embedding? \n",
    "TE = use pre-trained model to represent my data in N-dim\n",
    "RE = use pre-defined NN to train on my data and represent in som N-dim ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c326be9",
   "metadata": {},
   "source": [
    "# Notes for myself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074aba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T07:33:35.478757Z",
     "start_time": "2022-01-07T07:33:35.476368Z"
    }
   },
   "source": [
    "- !! some texts truncated example \"yeah....\" in sjekk\n",
    "- remove \"amp\"--> stands for &\n",
    "- Add confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc2d88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.415389Z",
     "start_time": "2022-01-07T15:24:09.415376Z"
    }
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6,6))\n",
    "# cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "# sns.heatmap(cm, square=True, annot=True, cbar=False,\n",
    "#             xticklabels=['non-hate', 'hate'], yticklabels=['non-hate', 'hate'])\n",
    "# plt.xlabel('Predicted label')\n",
    "# plt.ylabel('True label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0ce3e",
   "metadata": {},
   "source": [
    "## Transfer learning vs \"simple\" embedding\n",
    "\n",
    "- As mentionned ealier, Word2vec trains an internal Neural network whose goal is to predict a word in a corpus\n",
    "based on the words around it. This part of the sentence is called the window.\n",
    "Its size corresponds to the number of word around word W used to predict this word W\n",
    "\n",
    "- Instead of learning it on your training set (especially if it is very small), you can directly \n",
    "load a pretrained embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b118494",
   "metadata": {},
   "source": [
    "# Keras documentation\n",
    "\n",
    "- padding : https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "- embedding: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding#example_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5305e2c2",
   "metadata": {},
   "source": [
    "# Gensim word2vec documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befafaf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T07:41:20.963944Z",
     "start_time": "2022-01-07T07:41:20.960192Z"
    }
   },
   "source": [
    "- https://radimrehurek.com/gensim/\n",
    "- https://radimrehurek.com/gensim/models/word2vec.html#usage-examples\n",
    "- https://radimrehurek.com/gensim/models/word2vec.html#usage-examples\n",
    "- hyperparameters: https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Text8Corpus\n",
    "- transfer learning : gensim-data repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8666cc",
   "metadata": {},
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d53b07f",
   "metadata": {},
   "source": [
    "- twick Embedding hyperparameters to improve score\n",
    "- Understand the other layers and hyperparameters to tune\n",
    "- Improve data cleaning part (lemantizing?)\n",
    "- Run all on bigger datasett\n",
    "- ++++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf02db",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afc424b",
   "metadata": {},
   "source": [
    "### Results_ss100_undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1782366d",
   "metadata": {},
   "source": [
    "| model_name | Embedding_type | validation_accuracy | test_accuracy|\n",
    "| --- | --- | --- |---|\n",
    "| | from scratch | 0.4167 |0.55|\n",
    "| | w2v | 0.75 |0.65|\n",
    "|  | w2vTF | 0.7083 |0.70|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e02bf57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T15:24:09.416165Z",
     "start_time": "2022-01-07T15:24:09.416152Z"
    }
   },
   "outputs": [],
   "source": [
    "0.6667  60.500"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "352.823px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
