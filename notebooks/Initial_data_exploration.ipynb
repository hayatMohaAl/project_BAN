{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e06ad07",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd9352a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T16:26:33.581050Z",
     "start_time": "2022-01-05T16:26:32.010711Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import string \n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4160115a",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01123c62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T16:26:33.703010Z",
     "start_time": "2022-01-05T16:26:33.589219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8582\n",
       "1    1418\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', None) \n",
    "df = pd.read_csv(\"~/Downloads/banData/MeTooHate.csv\", nrows = 10000)\n",
    "#df.head()\n",
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9481629",
   "metadata": {},
   "source": [
    "# Sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf39189",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T16:25:53.961439Z",
     "start_time": "2022-01-05T16:25:53.955280Z"
    }
   },
   "source": [
    "## Annotation rule\n",
    "\n",
    "- 0: neutral  content or positive sentiment\n",
    "- 1: negative sentiment or negative fact but no abusive wording\n",
    "- 2: Abusive/ hate language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e261b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T16:26:34.636937Z",
     "start_time": "2022-01-05T16:26:34.586600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    58\n",
       "0    58\n",
       "2    23\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mini = df[df[\"category\"]==1].iloc[0:101].head(100)\n",
    "df_mini.reset_index(inplace =True)\n",
    "\n",
    "label=[2, 2, 1,  1, 1, 1,  1, 2,  1, 0,  2,   1,  1,  1,  1,\n",
    "  1, 1, 1,0,2,1,    1,    1,    1,    1,   2,     1,    2,    0,    1,\n",
    "  0,1,2, 0,1,2, 0,1,1,0, \n",
    "  1,0,1,0,1,1,1,2,1,2,\n",
    "  2,2,2,0,1, 0,2,1,1,1,\n",
    "  1,1,1,0,1,2,1,0,1,0,\n",
    "  2,  1,1,   2, 1,2,   1, 1,0,1,1 ,\n",
    "  1,   2,   2,   0,   2,   0,   1,   1,   1,   1,   1,   1,   0,\n",
    "   1,2,1, 0, 1,1]\n",
    "df_mini[\"label\"] = label\n",
    "\n",
    "tmp = df[df[\"category\"]==0].iloc[0:39]\n",
    "tmp.reset_index(inplace =True)\n",
    "tmp[\"label\"] = 0\n",
    "tmp.index = pd.Series(np.array(tmp.index)+100)\n",
    "\n",
    "df_mini = pd.concat([df_mini, tmp],  axis=0 )\n",
    "df_mini.drop(columns = [\"status_id\",'created_at', 'location'], inplace = True)\n",
    "df_mini.reset_index(inplace  = True)\n",
    "\n",
    "df_mini.drop_duplicates(inplace = True)\n",
    "df_mini.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e468080b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T16:51:52.347216Z",
     "start_time": "2022-01-04T16:51:52.329798Z"
    }
   },
   "source": [
    "# Litterature & github account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd0cfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T10:01:17.868248Z",
     "start_time": "2022-01-05T10:01:17.864586Z"
    }
   },
   "source": [
    "- label propagation:  https://towardsdatascience.com/semi-supervised-learning-how-to-assign-labels-with-label-propagation-algorithm-9f1683f4d0eb\n",
    "- Lexicon of abusive words: https://github.com/uds-lsv/lexicon-of-abusive-words     \n",
    "- Deep learning and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f07db7",
   "metadata": {},
   "source": [
    "# Alternative modeling plans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4393c22",
   "metadata": {},
   "source": [
    " - Manually relabel 2%-5% of tweets in category 1 (hate speech)\n",
    " - Apply label propagation algo to label the remaining tweets\n",
    " - All tweets labeled,  proceed with classification/DL model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117964fc",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf752b",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baad6db0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T16:26:42.360324Z",
     "start_time": "2022-01-05T16:26:41.484218Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mimi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mimi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mimi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/mimi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1743d6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T13:53:26.365143Z",
     "start_time": "2022-01-05T13:53:26.357586Z"
    }
   },
   "outputs": [],
   "source": [
    "text = hate_text.iloc[59]\n",
    "\n",
    "# lowercase\n",
    "text = text.lower() \n",
    "\n",
    "# remove numbers\n",
    "text = ''.join(word for word in text if not word.isdigit())\n",
    "\n",
    "# punctuation\n",
    "#string.punctuation\n",
    "for punctuation in string.punctuation:\n",
    "    text = text.replace(punctuation, '') \n",
    "    \n",
    "# stopwords\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e189839",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T13:53:26.611962Z",
     "start_time": "2022-01-05T13:53:26.594983Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizing - transform to list\n",
    "word_tokens = word_tokenize(text) \n",
    "text = [w for w in word_tokens if not w in stop_words] \n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60484e38",
   "metadata": {},
   "source": [
    "## Lemantizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3388188c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T13:53:28.034403Z",
     "start_time": "2022-01-05T13:53:26.997944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stemming or Lemmatizing- finding root word\n",
    "# stemmer = PorterStemmer()\n",
    "# stemmed = [stemmer.stem(word) for word in text]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in text]\n",
    "text = lemmatized\n",
    "lemmatized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7fa8db",
   "metadata": {},
   "source": [
    "## Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c436fa76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T13:53:28.042403Z",
     "start_time": "2022-01-05T13:53:28.035695Z"
    }
   },
   "outputs": [],
   "source": [
    "# text numerical ----> representation\n",
    "\n",
    "## Bag of words: counting ocuurences of each word (mx with word in col)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(text)\n",
    "X.toarray()   \n",
    "#vectorizer.get_feature_names()  \n",
    "#pd.DataFrame(X.toarray(),columns = vectorizer.get_feature_names())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ada646",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T13:53:28.050973Z",
     "start_time": "2022-01-05T13:53:28.044163Z"
    }
   },
   "outputs": [],
   "source": [
    "## Tf-Idf: Term Frequency - Inverse Document Frequency --> importance of word in document--> frequency\n",
    "# pros: - frequency robust to document length\n",
    "#       - measures importance\n",
    "# cons: doesn't capture context\n",
    "\n",
    "# texts = ['i love football',\n",
    "#          'football is a game i love',\n",
    "#         'football football football']\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "X = tf_idf_vectorizer.fit_transform(text)\n",
    "#X.toarray()\n",
    "#pd.DataFrame(X.toarray(),columns = tf_idf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f627b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T13:53:28.286607Z",
     "start_time": "2022-01-05T13:53:28.281395Z"
    }
   },
   "outputs": [],
   "source": [
    "# Key paramters BOW and Td-Idf :\n",
    "# max_df = exclude \"corpus specific stopwords\", most freq words\n",
    "# min_df = exclude words that are very infrequent in the dataset\n",
    "# max_features = specify the number of features to keep when vectorizing, useful to reduce the dimension of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0be026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T13:53:28.939378Z",
     "start_time": "2022-01-05T13:53:28.915342Z"
    }
   },
   "outputs": [],
   "source": [
    "### N-grams: Instead of considering individual words, \n",
    "#N-grams consists of considering word sequences. \n",
    "#This representation captures context. N is the number of words to be consiered as a one\n",
    "texts =  [\n",
    "         'i do not love football',\n",
    "         'i love football not basketball']\n",
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range = (2,2))\n",
    "\n",
    "X = tf_idf_vectorizer.fit_transform(texts)\n",
    "\n",
    "X.toarray()\n",
    "\n",
    "pd.DataFrame(X.toarray(),columns = tf_idf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7d68c4",
   "metadata": {},
   "source": [
    "# Test label propagation on minimal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494b625a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T13:53:30.425798Z",
     "start_time": "2022-01-05T13:53:29.394170Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad9feb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T13:53:31.423796Z",
     "start_time": "2022-01-05T13:53:30.429283Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import plotly.express as px # for data visualization\n",
    "import plotly.graph_objects as go # for data visualization\n",
    "import matplotlib.pyplot as plt # for displaying confusion matrix\n",
    "\n",
    "# Skleran\n",
    "from sklearn.metrics import classification_report # for model evaluation metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay # for showing confusion matrix\n",
    "from sklearn.preprocessing import MinMaxScaler # for feature scaling\n",
    "from sklearn.semi_supervised import LabelPropagation # for assigning labels to unlabeled data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82045e73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T16:27:59.463076Z",
     "start_time": "2022-01-05T16:27:59.456694Z"
    }
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase\n",
    "text = text.lower() \n",
    "\n",
    "# remove numbers\n",
    "text = ''.join(word for word in text if not word.isdigit())\n",
    "\n",
    "# punctuation\n",
    "#string.punctuation\n",
    "for punctuation in string.punctuation:\n",
    "    text = text.replace(punctuation, '') \n",
    "    \n",
    "# stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in text]\n",
    "text = lemmatized\n",
    "lemmatized\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "X = tf_idf_vectorizer.fit_transform(text)\n",
    "#X.toarray()\n",
    "#pd.DataFrame(X.toarray(),columns = tf_idf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48337cba",
   "metadata": {},
   "source": [
    "# External abusive words list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f289c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "abusive_words_df = pd.read_csv('../raw_data/abusive_words_list.txt', delimiter = \"\\t\").iloc[:,0:2]\n",
    "abusive_words_df.columns = [\"word\", \"neg_polarity\"]\n",
    "word2 = [str(x).split(\"_\")[0] for x in abusive_words_df.word]\n",
    "word2 = pd.Series(word2)\n",
    "word2\n",
    "abusive_words_df[\"word2\"] = word2\n",
    "abusive_words_df\n",
    "\n",
    "word3 = pd.DataFrame(abusive_words_df[\"word2\"].unique())\n",
    "len(word3)\n",
    "#'../raw_data/abusive_words_list.txt'\n",
    "word3.to_csv('../project_BAN/data/abusive_words.txt',header=\"words\", index=None, sep=' ', mode='a')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8bdee0",
   "metadata": {},
   "source": [
    "# Useful notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13653315",
   "metadata": {},
   "source": [
    "imoji heart in negative comment \n",
    "Because is true and powerful (thanks, ❤️), I'm unearthing so much of my past. I just remembered that time a platonic male friend came to my house with a fish bat, repeatedly brandishing it in a threatening manner, and how calm I had to remain to get him ou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff187a38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T12:49:44.825031Z",
     "start_time": "2022-01-05T12:49:44.819338Z"
    }
   },
   "source": [
    "## Some features are:\n",
    "\n",
    "- Vocabulary Richness\n",
    "- Number of words per tweet\n",
    "- punctuation/Character ratio\n",
    "- emoji/Character ratio\n",
    "- Contains abusive words\n",
    "- Contains Words in Capital letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7bb4ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T12:53:51.862515Z",
     "start_time": "2022-01-05T12:53:51.830705Z"
    }
   },
   "outputs": [],
   "source": [
    "def vocab_richness(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    total_length = len(tokens)\n",
    "    unique_words = set(tokens)\n",
    "    unique_word_length = len(unique_words)\n",
    "    return unique_word_length/total_length\n",
    "\n",
    "vocab_richness = df_mini.text.apply(vocab_richness)\n",
    "\n",
    "vocab_richness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780987c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T12:56:41.814864Z",
     "start_time": "2022-01-05T12:56:41.805881Z"
    }
   },
   "source": [
    "## Machine learning models\n",
    "- Naive base fro classification\n",
    "- LatentDirichletAllocation for clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c87c9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T13:09:34.903495Z",
     "start_time": "2022-01-05T13:09:34.898285Z"
    }
   },
   "source": [
    "## Deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2045366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T13:10:58.064962Z",
     "start_time": "2022-01-05T13:10:58.060060Z"
    }
   },
   "source": [
    "https://kitt.lewagon.com/camps/773/lectures/content/06-DL_05-Natural-Language-Processing.html\n",
    "    \n",
    "X.shape = (n_sentences, max_sentence_length, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840f420b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T13:16:57.493176Z",
     "start_time": "2022-01-05T13:16:57.486838Z"
    }
   },
   "source": [
    "steps:\n",
    "    - tokenize text: tf.keras.preprocessing.text.Tokenizer\n",
    "    - embedding layer (creating vector representation of each word): https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "                - custome embedding : can be done but computationally intensive\n",
    "                - independent embedding woth Word2vec (Gensim) --- transfer learning\n",
    "                \n",
    "Powerful embedding that is very fast and easy to train!\n",
    "\n",
    "✅ you give it a list of sentences\n",
    "✅ it automatically learns a representation - an embedding - for each word it was trained on!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c1716",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "393.143px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
